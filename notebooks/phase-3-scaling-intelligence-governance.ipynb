{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7412ae",
   "metadata": {},
   "source": [
    "# Phase III: Skalierung, Intelligenz & Governance\n",
    "## CiviCRM Starter-Suite f√ºr Menschlichkeit √ñsterreich\n",
    "\n",
    "**Version**: 3.0.0  \n",
    "**Datum**: Juni 2025  \n",
    "**Status**: Implementation Ready\n",
    "\n",
    "---\n",
    "\n",
    "## √úbersicht\n",
    "\n",
    "Phase III erweitert die erfolgreiche CiviCRM-Plattform um:\n",
    "\n",
    "1. **üóÑÔ∏è Daten-Backbone & Reporting-Layer** \n",
    "   - Event Store mit PostgreSQL + TimescaleDB\n",
    "   - ETL-Pipelines mit Airbyte/Meltano ‚Üí DuckDB\n",
    "   - BI-Layer mit Metabase\n",
    "   - OKR-f√§hige Kennzahlen mit dbt\n",
    "\n",
    "2. **üìä Quick-Win-Dashboards**\n",
    "   - Funnel-Analyse (Lead ‚Üí Member)\n",
    "   - CLV Heat-Map (Customer Lifetime Value)\n",
    "   - Churn Radar (Engagement-Score Monitoring)\n",
    "\n",
    "3. **ü§ñ KI-gest√ºtzte Personalisierung**\n",
    "   - Propensity-Modelle f√ºr Mitgliedschaftswahrscheinlichkeit\n",
    "   - Send-Time-Optimierung mit Prophet\n",
    "   - Copy-Fine-Tuning mit OpenAI API\n",
    "\n",
    "4. **üë• Volunteer-Lifecycle (F-19 bis F-22)**\n",
    "   - Erweiterte n8n Workflows f√ºr Ehrenamt\n",
    "   - Skill-Matching mit Airtable\n",
    "   - OpenBadges-Integration\n",
    "\n",
    "5. **üõ°Ô∏è Governance, Risk & Compliance**\n",
    "   - Automatisierte Sicherheits-Checks\n",
    "   - DSGVO-Compliance-Monitoring\n",
    "   - Incident-Response-Automation\n",
    "\n",
    "6. **üöÄ Continuous Improvement**\n",
    "   - SLO-Monitoring mit Prometheus\n",
    "   - Automatisierte Roadmap-Verfolgung\n",
    "   - Data-driven Decision Making\n",
    "\n",
    "---\n",
    "\n",
    "**Zielsetzung**: Transformation von einer automatisierten zu einer **datengetriebenen** und **selbst-optimierenden** Plattform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d163a",
   "metadata": {},
   "source": [
    "# 1. Daten-Backbone & Reporting-Layer\n",
    "\n",
    "## Event Store Setup mit PostgreSQL + TimescaleDB\n",
    "\n",
    "Der Event Store bildet das Herzst√ºck unserer datengetriebenen Architektur. Alle Ereignisse aus CiviCRM, n8n und FreeFinance werden hier revisionssicher gespeichert.\n",
    "\n",
    "### Architektur-Komponenten:\n",
    "- **Event Store**: PostgreSQL + TimescaleDB f√ºr Zeitreihen-Optimierung\n",
    "- **ETL Pipeline**: Airbyte/Meltano ‚Üí DuckDB Staging ‚Üí dbt Transformation  \n",
    "- **BI Layer**: Metabase f√ºr Self-Service-Dashboards\n",
    "- **Kennzahlen**: dbt-Models f√ºr OKR-Tracking\n",
    "\n",
    "### Retention Policy:\n",
    "- **7 Jahre** f√ºr Compliance-Daten (DSGVO)\n",
    "- **Write-ahead-log** Replikation f√ºr Ausfallsicherheit\n",
    "- **Schema-Versionierung** mit dbt f√ºr evolution√§re Datenmodelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1.1 EVENT STORE SETUP - PostgreSQL + TimescaleDB\n",
    "# =============================================================================\n",
    "\n",
    "def setup_event_store():\n",
    "    \"\"\"\n",
    "    Event Store Schema f√ºr CiviCRM Starter-Suite\n",
    "    Revisionssichere Speicherung aller Ereignisse mit TimescaleDB-Optimierung\n",
    "    \"\"\"\n",
    "    \n",
    "    # Event Store Schema Definition\n",
    "    event_store_schema = \"\"\"\n",
    "    -- Event Store f√ºr Phase III\n",
    "    CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;\n",
    "    \n",
    "    -- Core Event Table\n",
    "    CREATE TABLE IF NOT EXISTS events (\n",
    "        event_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "        event_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "        event_type VARCHAR(50) NOT NULL,\n",
    "        entity_type VARCHAR(50) NOT NULL,  -- contact, contribution, membership, etc.\n",
    "        entity_id INTEGER NOT NULL,\n",
    "        source_system VARCHAR(20) NOT NULL,  -- civicrm, n8n, freeFinance\n",
    "        event_data JSONB NOT NULL,\n",
    "        correlation_id UUID,  -- f√ºr Workflow-Tracking\n",
    "        user_id INTEGER,\n",
    "        created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "    );\n",
    "    \n",
    "    -- TimescaleDB Hypertable f√ºr Performance\n",
    "    SELECT create_hypertable('events', 'event_time', if_not_exists => TRUE);\n",
    "    \n",
    "    -- Indexes f√ºr optimale Performance\n",
    "    CREATE INDEX IF NOT EXISTS idx_events_type_time ON events (event_type, event_time DESC);\n",
    "    CREATE INDEX IF NOT EXISTS idx_events_entity ON events (entity_type, entity_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_events_source ON events (source_system, event_time DESC);\n",
    "    CREATE INDEX IF NOT EXISTS idx_events_correlation ON events (correlation_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_events_data_gin ON events USING GIN (event_data);\n",
    "    \n",
    "    -- Retention Policy (7 Jahre f√ºr DSGVO)\n",
    "    SELECT add_retention_policy('events', INTERVAL '7 years', if_not_exists => TRUE);\n",
    "    \n",
    "    -- Views f√ºr h√§ufige Abfragen\n",
    "    CREATE OR REPLACE VIEW contact_events AS\n",
    "    SELECT * FROM events WHERE entity_type = 'contact';\n",
    "    \n",
    "    CREATE OR REPLACE VIEW contribution_events AS\n",
    "    SELECT * FROM events WHERE entity_type = 'contribution';\n",
    "    \n",
    "    CREATE OR REPLACE VIEW membership_events AS\n",
    "    SELECT * FROM events WHERE entity_type = 'membership';\n",
    "    \n",
    "    -- Materialized Views f√ºr Performance\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS daily_event_summary AS\n",
    "    SELECT \n",
    "        DATE(event_time) as event_date,\n",
    "        event_type,\n",
    "        source_system,\n",
    "        COUNT(*) as event_count,\n",
    "        COUNT(DISTINCT entity_id) as unique_entities\n",
    "    FROM events \n",
    "    WHERE event_time >= NOW() - INTERVAL '30 days'\n",
    "    GROUP BY DATE(event_time), event_type, source_system;\n",
    "    \n",
    "    CREATE UNIQUE INDEX ON daily_event_summary (event_date, event_type, source_system);\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä Event Store Schema Definition:\")\n",
    "    print(event_store_schema)\n",
    "    \n",
    "    return event_store_schema\n",
    "\n",
    "# Event Store Connection & Setup\n",
    "try:\n",
    "    engine = create_engine(POSTGRES_URL)\n",
    "    \n",
    "    # Test Connection\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\"SELECT version()\").fetchone()\n",
    "        print(f\"‚úÖ PostgreSQL Connection successful: {result[0][:50]}...\")\n",
    "    \n",
    "    schema = setup_event_store()\n",
    "    print(\"üèóÔ∏è Event Store Schema ready for deployment\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è PostgreSQL Connection failed: {e}\")\n",
    "    print(\"üí° Using mock data for demonstration purposes\")\n",
    "\n",
    "# Mock Event Data f√ºr Demonstration\n",
    "sample_events = [\n",
    "    {\n",
    "        'event_type': 'contribution_created',\n",
    "        'entity_type': 'contribution',\n",
    "        'entity_id': 12345,\n",
    "        'source_system': 'civicrm',\n",
    "        'event_data': {\n",
    "            'amount': 250.00,\n",
    "            'currency': 'EUR',\n",
    "            'contact_id': 9876,\n",
    "            'campaign_id': 42,\n",
    "            'payment_method': 'sepa'\n",
    "        },\n",
    "        'correlation_id': 'wf-f01-donation-2025-06-21-001'\n",
    "    },\n",
    "    {\n",
    "        'event_type': 'membership_created',\n",
    "        'entity_type': 'membership',\n",
    "        'entity_id': 5678,\n",
    "        'source_system': 'n8n',\n",
    "        'event_data': {\n",
    "            'contact_id': 1234,\n",
    "            'membership_type': 'regular',\n",
    "            'status': 'new',\n",
    "            'join_date': '2025-06-21',\n",
    "            'workflow': 'F-12_membership_apply'\n",
    "        },\n",
    "        'correlation_id': 'wf-f12-member-2025-06-21-002'\n",
    "    }\n",
    "]\n",
    "\n",
    "events_df = pd.DataFrame(sample_events)\n",
    "print(\"üìã Sample Events:\")\n",
    "print(events_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a951d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1.2 ETL PIPELINE - DuckDB Staging & dbt Transformation\n",
    "# =============================================================================\n",
    "\n",
    "def setup_etl_pipeline():\n",
    "    \"\"\"\n",
    "    ETL Pipeline f√ºr CiviCRM ‚Üí DuckDB ‚Üí dbt ‚Üí Metabase\n",
    "    N√§chtliche Synchronisation aller Datenquellen\n",
    "    \"\"\"\n",
    "    \n",
    "    # DuckDB Setup f√ºr Staging\n",
    "    conn = duckdb.connect(DUCKDB_PATH)\n",
    "    \n",
    "    # Staging Tables erstellen\n",
    "    staging_sql = \"\"\"\n",
    "    -- CiviCRM Staging Tables\n",
    "    CREATE SCHEMA IF NOT EXISTS staging;\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS staging.contacts (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        display_name VARCHAR,\n",
    "        first_name VARCHAR,\n",
    "        last_name VARCHAR,\n",
    "        email VARCHAR,\n",
    "        phone VARCHAR,\n",
    "        created_date TIMESTAMP,\n",
    "        modified_date TIMESTAMP,\n",
    "        contact_type VARCHAR,\n",
    "        contact_sub_type VARCHAR,\n",
    "        is_deleted BOOLEAN DEFAULT FALSE,\n",
    "        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS staging.contributions (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        contact_id INTEGER,\n",
    "        total_amount DECIMAL(10,2),\n",
    "        currency VARCHAR(3),\n",
    "        contribution_status_id INTEGER,\n",
    "        financial_type_id INTEGER,\n",
    "        payment_instrument_id INTEGER,\n",
    "        receive_date TIMESTAMP,\n",
    "        created_date TIMESTAMP,\n",
    "        campaign_id INTEGER,\n",
    "        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS staging.memberships (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        contact_id INTEGER,\n",
    "        membership_type_id INTEGER,\n",
    "        status_id INTEGER,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        join_date DATE,\n",
    "        created_date TIMESTAMP,\n",
    "        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \n",
    "    -- n8n Workflow Executions\n",
    "    CREATE TABLE IF NOT EXISTS staging.workflow_executions (\n",
    "        id VARCHAR PRIMARY KEY,\n",
    "        workflow_id VARCHAR,\n",
    "        execution_status VARCHAR,\n",
    "        started_at TIMESTAMP,\n",
    "        finished_at TIMESTAMP,\n",
    "        data STRUCT,\n",
    "        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \n",
    "    -- Analytics Schema f√ºr transformierte Daten\n",
    "    CREATE SCHEMA IF NOT EXISTS analytics;\n",
    "    \"\"\"\n",
    "    \n",
    "    conn.execute(staging_sql)\n",
    "    print(\"‚úÖ DuckDB Staging Tables created\")\n",
    "    \n",
    "    # dbt Models Definition\n",
    "    dbt_models = {\n",
    "        'member_funnel': \"\"\"\n",
    "        {{ config(materialized='table') }}\n",
    "        \n",
    "        WITH lead_contacts AS (\n",
    "            SELECT \n",
    "                c.id,\n",
    "                c.display_name,\n",
    "                c.email,\n",
    "                c.created_date as lead_date,\n",
    "                CASE \n",
    "                    WHEN m.id IS NOT NULL THEN 'converted'\n",
    "                    ELSE 'lead'\n",
    "                END as status\n",
    "            FROM {{ ref('staging_contacts') }} c\n",
    "            LEFT JOIN {{ ref('staging_memberships') }} m ON c.id = m.contact_id\n",
    "        ),\n",
    "        \n",
    "        funnel_metrics AS (\n",
    "            SELECT \n",
    "                DATE_TRUNC('month', lead_date) as month,\n",
    "                COUNT(*) as total_leads,\n",
    "                COUNT(CASE WHEN status = 'converted' THEN 1 END) as conversions,\n",
    "                ROUND(\n",
    "                    COUNT(CASE WHEN status = 'converted' THEN 1 END) * 100.0 / COUNT(*), \n",
    "                    2\n",
    "                ) as conversion_rate\n",
    "            FROM lead_contacts\n",
    "            GROUP BY DATE_TRUNC('month', lead_date)\n",
    "        )\n",
    "        \n",
    "        SELECT * FROM funnel_metrics\n",
    "        ORDER BY month DESC\n",
    "        \"\"\",\n",
    "        \n",
    "        'clv_analysis': \"\"\"\n",
    "        {{ config(materialized='table') }}\n",
    "        \n",
    "        WITH customer_metrics AS (\n",
    "            SELECT \n",
    "                c.id as contact_id,\n",
    "                c.display_name,\n",
    "                MIN(contrib.receive_date) as first_contribution,\n",
    "                MAX(contrib.receive_date) as last_contribution,\n",
    "                COUNT(contrib.id) as total_contributions,\n",
    "                SUM(contrib.total_amount) as total_contributed,\n",
    "                AVG(contrib.total_amount) as avg_contribution,\n",
    "                COUNT(DISTINCT DATE_TRUNC('month', contrib.receive_date)) as active_months,\n",
    "                CASE \n",
    "                    WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '90 days' THEN 'active'\n",
    "                    WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '365 days' THEN 'at_risk'\n",
    "                    ELSE 'churned'\n",
    "                END as customer_segment\n",
    "            FROM {{ ref('staging_contacts') }} c\n",
    "            JOIN {{ ref('staging_contributions') }} contrib ON c.id = contrib.contact_id\n",
    "            GROUP BY c.id, c.display_name\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            *,\n",
    "            CASE \n",
    "                WHEN total_contributed >= 1000 THEN 'high_value'\n",
    "                WHEN total_contributed >= 250 THEN 'medium_value'\n",
    "                ELSE 'low_value'\n",
    "            END as value_segment,\n",
    "            total_contributed / NULLIF(active_months, 0) as monthly_clv\n",
    "        FROM customer_metrics\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    print(\"üìä dbt Models defined:\")\n",
    "    for model, sql in dbt_models.items():\n",
    "        print(f\"  - {model}\")\n",
    "    \n",
    "    # Mock Data f√ºr Demonstration\n",
    "    mock_contacts = pd.DataFrame({\n",
    "        'id': range(1, 251),\n",
    "        'display_name': [f'Contact {i}' for i in range(1, 251)],\n",
    "        'email': [f'contact{i}@example.org' for i in range(1, 251)],\n",
    "        'created_date': pd.date_range('2024-01-01', periods=250, freq='D'),\n",
    "        'contact_type': 'Individual'\n",
    "    })\n",
    "    \n",
    "    mock_contributions = pd.DataFrame({\n",
    "        'id': range(1, 501),\n",
    "        'contact_id': np.random.choice(range(1, 251), 500),\n",
    "        'total_amount': np.random.normal(150, 100, 500).clip(10, 1000),\n",
    "        'currency': 'EUR',\n",
    "        'receive_date': pd.date_range('2024-01-01', periods=500, freq='2D')\n",
    "    })\n",
    "    \n",
    "    # Load Mock Data in DuckDB\n",
    "    conn.register('contacts_df', mock_contacts)\n",
    "    conn.register('contributions_df', mock_contributions)\n",
    "    \n",
    "    conn.execute(\"INSERT INTO staging.contacts SELECT * FROM contacts_df\")\n",
    "    conn.execute(\"INSERT INTO staging.contributions SELECT * FROM contributions_df\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(mock_contacts)} contacts and {len(mock_contributions)} contributions\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "# ETL Pipeline ausf√ºhren\n",
    "etl_conn = setup_etl_pipeline()\n",
    "\n",
    "# Beispiel-Analytics Query\n",
    "analytics_query = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('month', receive_date) as month,\n",
    "    COUNT(*) as contributions_count,\n",
    "    SUM(total_amount) as total_amount,\n",
    "    AVG(total_amount) as avg_amount,\n",
    "    COUNT(DISTINCT contact_id) as unique_donors\n",
    "FROM staging.contributions \n",
    "GROUP BY DATE_TRUNC('month', receive_date)\n",
    "ORDER BY month DESC\n",
    "LIMIT 12\n",
    "\"\"\"\n",
    "\n",
    "monthly_stats = etl_conn.execute(analytics_query).df()\n",
    "print(\"üìà Monthly Contribution Analytics:\")\n",
    "print(monthly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e3065",
   "metadata": {},
   "source": [
    "# 2. Quick-Win Dashboards\n",
    "\n",
    "Diese Dashboards liefern sofortige Insights f√ºr strategische Entscheidungen und operatives Management.\n",
    "\n",
    "## üéØ Dashboard-Portfolio:\n",
    "\n",
    "### 1. **Funnel-Analyse** \n",
    "*Lead ‚Üí Pending ‚Üí New Member* mit Tages- und Quartals-Drill-downs\n",
    "\n",
    "### 2. **CLV Heat-Map** \n",
    "*Customer Lifetime Value* kombiniert Spenden + Mitgliedsbeitr√§ge pro Kontakt-Kohorte\n",
    "\n",
    "### 3. **Churn Radar** \n",
    "*Engagement-Score < 30 & Renewal ‚â§ 60 Tage* f√ºr proaktive Retention\n",
    "\n",
    "Alle Dashboards sind **Metabase-ready** und k√∂nnen direkt in das Board-Dashboard integriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.1 FUNNEL ANALYSIS - Lead to Member Conversion\n",
    "# =============================================================================\n",
    "\n",
    "def create_funnel_analysis():\n",
    "    \"\"\"\n",
    "    Funnel-Analyse: Lead ‚Üí Interest ‚Üí Application ‚Üí New Member\n",
    "    Mit Conversion-Rates und Zeitanalyse\n",
    "    \"\"\"\n",
    "    \n",
    "    # Funnel-Daten aus ETL Pipeline\n",
    "    funnel_query = \"\"\"\n",
    "    WITH lead_journey AS (\n",
    "        SELECT \n",
    "            c.id,\n",
    "            c.display_name,\n",
    "            c.created_date as lead_date,\n",
    "            MIN(contrib.receive_date) as first_contribution,\n",
    "            MIN(m.join_date) as membership_date,\n",
    "            CASE \n",
    "                WHEN m.id IS NOT NULL THEN 'member'\n",
    "                WHEN contrib.id IS NOT NULL THEN 'donor'\n",
    "                ELSE 'lead'\n",
    "            END as current_status,\n",
    "            CASE \n",
    "                WHEN m.join_date IS NOT NULL THEN \n",
    "                    EXTRACT(DAY FROM m.join_date - c.created_date)\n",
    "                ELSE NULL\n",
    "            END as days_to_conversion\n",
    "        FROM staging.contacts c\n",
    "        LEFT JOIN staging.contributions contrib ON c.id = contrib.contact_id\n",
    "        LEFT JOIN staging.memberships m ON c.id = m.contact_id\n",
    "        WHERE c.created_date >= '2024-01-01'\n",
    "        GROUP BY c.id, c.display_name, c.created_date\n",
    "    ),\n",
    "    \n",
    "    funnel_stages AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', lead_date) as month,\n",
    "            COUNT(*) as total_leads,\n",
    "            COUNT(CASE WHEN first_contribution IS NOT NULL THEN 1 END) as engaged_leads,\n",
    "            COUNT(CASE WHEN current_status = 'member' THEN 1 END) as new_members,\n",
    "            AVG(days_to_conversion) as avg_conversion_days\n",
    "        FROM lead_journey\n",
    "        GROUP BY DATE_TRUNC('month', lead_date)\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        month,\n",
    "        total_leads,\n",
    "        engaged_leads,\n",
    "        new_members,\n",
    "        ROUND(engaged_leads * 100.0 / total_leads, 1) as engagement_rate,\n",
    "        ROUND(new_members * 100.0 / total_leads, 1) as conversion_rate,\n",
    "        ROUND(new_members * 100.0 / NULLIF(engaged_leads, 0), 1) as member_conversion_rate,\n",
    "        ROUND(avg_conversion_days, 1) as avg_days_to_member\n",
    "    FROM funnel_stages\n",
    "    ORDER BY month DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    funnel_data = etl_conn.execute(funnel_query).df()\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('üìä Mitglieder-Funnel Analyse - Phase III', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Funnel Volumen\n",
    "    months = funnel_data['month'].dt.strftime('%Y-%m')\n",
    "    ax1.bar(months, funnel_data['total_leads'], alpha=0.7, label='Total Leads', color='lightblue')\n",
    "    ax1.bar(months, funnel_data['engaged_leads'], alpha=0.8, label='Engaged', color='orange')\n",
    "    ax1.bar(months, funnel_data['new_members'], alpha=0.9, label='New Members', color='green')\n",
    "    ax1.set_title('Funnel Volumen')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Conversion Rates\n",
    "    ax2.plot(months, funnel_data['engagement_rate'], marker='o', label='Engagement Rate %', color='orange')\n",
    "    ax2.plot(months, funnel_data['conversion_rate'], marker='s', label='Member Conversion %', color='green')\n",
    "    ax2.set_title('Conversion Rates')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Funnel Efficiency\n",
    "    efficiency = funnel_data['member_conversion_rate'].fillna(0)\n",
    "    colors = ['red' if x < 10 else 'orange' if x < 20 else 'green' for x in efficiency]\n",
    "    ax3.bar(months, efficiency, color=colors, alpha=0.7)\n",
    "    ax3.set_title('Member Conversion Efficiency (Engaged ‚Üí Member)')\n",
    "    ax3.set_ylabel('Conversion Rate %')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Time to Conversion\n",
    "    avg_days = funnel_data['avg_days_to_member'].fillna(0)\n",
    "    ax4.bar(months, avg_days, color='purple', alpha=0.7)\n",
    "    ax4.set_title('Average Days: Lead ‚Üí Member')\n",
    "    ax4.set_ylabel('Days')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return funnel_data\n",
    "\n",
    "# Funnel-Analyse ausf√ºhren\n",
    "funnel_results = create_funnel_analysis()\n",
    "print(\"\\nüìà Funnel Analysis Results:\")\n",
    "print(funnel_results.to_string(index=False))\n",
    "\n",
    "# KPIs berechnen\n",
    "latest_month = funnel_results.iloc[0]\n",
    "print(f\"\\nüéØ Aktuelle Funnel-KPIs (neuester Monat):\")\n",
    "print(f\"   Leads: {latest_month['total_leads']}\")\n",
    "print(f\"   Engagement Rate: {latest_month['engagement_rate']}%\")\n",
    "print(f\"   Member Conversion: {latest_month['conversion_rate']}%\")\n",
    "print(f\"   Durchschnittliche Conversion-Zeit: {latest_month['avg_days_to_member']} Tage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2 CLV HEAT-MAP & CHURN RADAR\n",
    "# =============================================================================\n",
    "\n",
    "def create_clv_heatmap():\n",
    "    \"\"\"\n",
    "    Customer Lifetime Value Heat-Map\n",
    "    Kombination aus Spenden + Mitgliedsbeitr√§ge pro Kontakt-Kohorte\n",
    "    \"\"\"\n",
    "    \n",
    "    clv_query = \"\"\"\n",
    "    WITH customer_cohorts AS (\n",
    "        SELECT \n",
    "            c.id,\n",
    "            c.display_name,\n",
    "            DATE_TRUNC('quarter', c.created_date) as acquisition_quarter,\n",
    "            EXTRACT(YEAR FROM c.created_date) as acquisition_year,\n",
    "            COUNT(contrib.id) as total_contributions,\n",
    "            COALESCE(SUM(contrib.total_amount), 0) as total_contributed,\n",
    "            COUNT(DISTINCT DATE_TRUNC('month', contrib.receive_date)) as active_months,\n",
    "            MAX(contrib.receive_date) as last_contribution,\n",
    "            CASE \n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '90 days' THEN 'active'\n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '365 days' THEN 'at_risk'\n",
    "                ELSE 'churned'\n",
    "            END as lifecycle_stage\n",
    "        FROM staging.contacts c\n",
    "        LEFT JOIN staging.contributions contrib ON c.id = contrib.contact_id\n",
    "        WHERE c.created_date >= '2024-01-01'\n",
    "        GROUP BY c.id, c.display_name, c.created_date\n",
    "    ),\n",
    "    \n",
    "    cohort_analysis AS (\n",
    "        SELECT \n",
    "            acquisition_quarter,\n",
    "            lifecycle_stage,\n",
    "            COUNT(*) as customer_count,\n",
    "            AVG(total_contributed) as avg_clv,\n",
    "            SUM(total_contributed) as total_clv,\n",
    "            AVG(active_months) as avg_active_months\n",
    "        FROM customer_cohorts\n",
    "        GROUP BY acquisition_quarter, lifecycle_stage\n",
    "    )\n",
    "    \n",
    "    SELECT * FROM cohort_analysis\n",
    "    ORDER BY acquisition_quarter, lifecycle_stage\n",
    "    \"\"\"\n",
    "    \n",
    "    clv_data = etl_conn.execute(clv_query).df()\n",
    "    \n",
    "    # Heat-Map erstellen\n",
    "    pivot_data = clv_data.pivot(index='acquisition_quarter', \n",
    "                               columns='lifecycle_stage', \n",
    "                               values='avg_clv').fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "                cbar_kws={'label': 'Durchschnittlicher CLV (‚Ç¨)'})\n",
    "    plt.title('üî• Customer Lifetime Value Heat-Map\\nNach Akquisitions-Quartal und Lifecycle-Stage')\n",
    "    plt.xlabel('Lifecycle Stage')\n",
    "    plt.ylabel('Akquisitions-Quartal')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return clv_data\n",
    "\n",
    "def create_churn_radar():\n",
    "    \"\"\"\n",
    "    Churn Radar: Engagement-Score < 30 & Renewal ‚â§ 60 Tage\n",
    "    Proaktive Retention-Warnungen\n",
    "    \"\"\"\n",
    "    \n",
    "    churn_query = \"\"\"\n",
    "    WITH engagement_scores AS (\n",
    "        SELECT \n",
    "            c.id,\n",
    "            c.display_name,\n",
    "            c.email,\n",
    "            -- Engagement Score Berechnung (0-100)\n",
    "            CASE \n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '30 days' THEN 40\n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '90 days' THEN 25\n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '180 days' THEN 10\n",
    "                ELSE 0\n",
    "            END +\n",
    "            CASE \n",
    "                WHEN COUNT(contrib.id) >= 10 THEN 30\n",
    "                WHEN COUNT(contrib.id) >= 5 THEN 20\n",
    "                WHEN COUNT(contrib.id) >= 1 THEN 10\n",
    "                ELSE 0\n",
    "            END +\n",
    "            CASE \n",
    "                WHEN AVG(contrib.total_amount) >= 200 THEN 30\n",
    "                WHEN AVG(contrib.total_amount) >= 100 THEN 20\n",
    "                WHEN AVG(contrib.total_amount) >= 50 THEN 10\n",
    "                ELSE 0\n",
    "            END as engagement_score,\n",
    "            \n",
    "            MAX(contrib.receive_date) as last_contribution,\n",
    "            COUNT(contrib.id) as contribution_count,\n",
    "            AVG(contrib.total_amount) as avg_contribution,\n",
    "            SUM(contrib.total_amount) as total_contributed,\n",
    "            \n",
    "            -- Membership Renewal Check\n",
    "            CASE \n",
    "                WHEN m.end_date IS NOT NULL AND m.end_date <= CURRENT_DATE + INTERVAL '60 days'\n",
    "                THEN m.end_date\n",
    "                ELSE NULL\n",
    "            END as renewal_due\n",
    "            \n",
    "        FROM staging.contacts c\n",
    "        LEFT JOIN staging.contributions contrib ON c.id = contrib.contact_id\n",
    "        LEFT JOIN staging.memberships m ON c.id = m.contact_id AND m.status_id = 1  -- Active memberships\n",
    "        GROUP BY c.id, c.display_name, c.email, m.end_date\n",
    "    ),\n",
    "    \n",
    "    churn_risk AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE \n",
    "                WHEN engagement_score <= 20 AND renewal_due IS NOT NULL THEN 'HIGH'\n",
    "                WHEN engagement_score <= 30 OR renewal_due IS NOT NULL THEN 'MEDIUM'\n",
    "                WHEN engagement_score <= 50 THEN 'LOW'\n",
    "                ELSE 'SAFE'\n",
    "            END as churn_risk_level,\n",
    "            \n",
    "            CASE \n",
    "                WHEN last_contribution < CURRENT_DATE - INTERVAL '90 days' THEN 'inactive'\n",
    "                WHEN contribution_count = 0 THEN 'non_contributor'\n",
    "                ELSE 'active'\n",
    "            END as activity_status\n",
    "        FROM engagement_scores\n",
    "    )\n",
    "    \n",
    "    SELECT * FROM churn_risk\n",
    "    WHERE churn_risk_level IN ('HIGH', 'MEDIUM')\n",
    "    ORDER BY churn_risk_level DESC, engagement_score ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    churn_data = etl_conn.execute(churn_query).df()\n",
    "    \n",
    "    # Churn Risk Visualisierung\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Risk Level Distribution\n",
    "    risk_counts = churn_data['churn_risk_level'].value_counts()\n",
    "    colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow'}\n",
    "    ax1.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
    "            colors=[colors.get(x, 'gray') for x in risk_counts.index])\n",
    "    ax1.set_title('üö® Churn Risk Distribution')\n",
    "    \n",
    "    # Engagement Score vs. Churn Risk\n",
    "    risk_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow', 'SAFE': 'green'}\n",
    "    for risk_level in churn_data['churn_risk_level'].unique():\n",
    "        subset = churn_data[churn_data['churn_risk_level'] == risk_level]\n",
    "        ax2.scatter(subset['engagement_score'], subset['total_contributed'], \n",
    "                   label=risk_level, color=risk_colors.get(risk_level, 'gray'), alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Engagement Score')\n",
    "    ax2.set_ylabel('Total Contributed (‚Ç¨)')\n",
    "    ax2.set_title('üìä Engagement vs. Contribution Value')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return churn_data\n",
    "\n",
    "# CLV Heat-Map erstellen\n",
    "print(\"üî• Creating CLV Heat-Map...\")\n",
    "clv_results = create_clv_heatmap()\n",
    "\n",
    "# Churn Radar erstellen\n",
    "print(\"\\nüö® Creating Churn Radar...\")\n",
    "churn_results = create_churn_radar()\n",
    "\n",
    "print(f\"\\nüìä Dashboard-Ergebnisse:\")\n",
    "print(f\"   CLV-Segmente: {len(clv_results)} Kohorten analysiert\")\n",
    "print(f\"   Churn-Risiko: {len(churn_results)} Kontakte ben√∂tigen Aufmerksamkeit\")\n",
    "\n",
    "if len(churn_results) > 0:\n",
    "    high_risk = len(churn_results[churn_results['churn_risk_level'] == 'HIGH'])\n",
    "    medium_risk = len(churn_results[churn_results['churn_risk_level'] == 'MEDIUM'])\n",
    "    print(f\"   üî¥ HIGH Risk: {high_risk} Kontakte\")\n",
    "    print(f\"   üü° MEDIUM Risk: {medium_risk} Kontakte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f0923",
   "metadata": {},
   "source": [
    "# 3. KI-gest√ºtzte Personalisierung\n",
    "\n",
    "## ü§ñ Machine Learning Models f√ºr optimierte Mitglieder-Journey\n",
    "\n",
    "Phase III f√ºhrt intelligente Personalisierung durch KI-Modelle ein:\n",
    "\n",
    "### üéØ **Propensity Model**\n",
    "- **Input**: `age`, `geo`, `engagement_score`, `donation_frequency`\n",
    "- **Output**: Wahrscheinlichkeit f√ºr Mitgliedschafts-Konversion\n",
    "- **Integration**: Jupyter ‚Üí ONNX ‚Üí n8n *Custom Code Node*\n",
    "\n",
    "### ‚è∞ **Send-Time Optimizer**  \n",
    "- **Input**: Historische √ñffnungs-/Klick-Zeitstempel\n",
    "- **Output**: Optimale Versandstunde (0-23) pro Kontakt\n",
    "- **Technik**: Python + Prophet ‚Üí Custom Field `best_send_hour`\n",
    "\n",
    "### ‚úçÔ∏è **Copy Fine-Tuning**\n",
    "- **Input**: Historische √ñffnungsraten vs. E-Mail-Betreff\n",
    "- **Output**: A/B-Test Betreff-Varianten\n",
    "- **Integration**: OpenAI Fine-tune API ‚Üí Template-Snippet-Repository\n",
    "\n",
    "### üîÑ **Automatischer Feedback-Loop**\n",
    "W√∂chentliche Aktualisierung der Trainingsdaten via `MailingSummary.get` f√ºr kontinuierliche Verbesserung aller Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 PROPENSITY MODEL - Mitgliedschafts-Wahrscheinlichkeit\n",
    "# =============================================================================\n",
    "\n",
    "def create_propensity_model():\n",
    "    \"\"\"\n",
    "    Machine Learning Model zur Vorhersage der Mitgliedschafts-Wahrscheinlichkeit\n",
    "    Features: age, geo, engagement_score, donation_frequency\n",
    "    Output: Probability Score (0-1) f√ºr Conversion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training Data vorbereiten\n",
    "    training_query = \"\"\"\n",
    "    WITH feature_engineering AS (\n",
    "        SELECT \n",
    "            c.id,\n",
    "            -- Age (falls verf√ºgbar, sonst gesch√§tzt)\n",
    "            COALESCE(\n",
    "                EXTRACT(YEAR FROM CURRENT_DATE) - EXTRACT(YEAR FROM c.birth_date),\n",
    "                CASE \n",
    "                    WHEN c.created_date < '2020-01-01' THEN 45 + RANDOM() * 30\n",
    "                    ELSE 35 + RANDOM() * 40\n",
    "                END\n",
    "            ) as age,\n",
    "            \n",
    "            -- Geography (Mock: PLZ-basiert)\n",
    "            CASE \n",
    "                WHEN RANDOM() < 0.3 THEN 'urban'\n",
    "                WHEN RANDOM() < 0.6 THEN 'suburban' \n",
    "                ELSE 'rural'\n",
    "            END as geo_type,\n",
    "            \n",
    "            -- Engagement Score (wie in Churn Radar)\n",
    "            CASE \n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '30 days' THEN 40\n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '90 days' THEN 25\n",
    "                WHEN MAX(contrib.receive_date) >= CURRENT_DATE - INTERVAL '180 days' THEN 10\n",
    "                ELSE 0\n",
    "            END +\n",
    "            CASE \n",
    "                WHEN COUNT(contrib.id) >= 10 THEN 30\n",
    "                WHEN COUNT(contrib.id) >= 5 THEN 20\n",
    "                WHEN COUNT(contrib.id) >= 1 THEN 10\n",
    "                ELSE 0\n",
    "            END as engagement_score,\n",
    "            \n",
    "            -- Donation Frequency (Spenden pro Monat)\n",
    "            COUNT(contrib.id) / GREATEST(\n",
    "                EXTRACT(MONTH FROM CURRENT_DATE - MIN(c.created_date)), 1\n",
    "            ) as donation_frequency,\n",
    "            \n",
    "            AVG(contrib.total_amount) as avg_donation,\n",
    "            \n",
    "            -- Target Variable: Ist Mitglied\n",
    "            CASE WHEN m.id IS NOT NULL THEN 1 ELSE 0 END as is_member\n",
    "            \n",
    "        FROM staging.contacts c\n",
    "        LEFT JOIN staging.contributions contrib ON c.id = contrib.contact_id\n",
    "        LEFT JOIN staging.memberships m ON c.id = m.contact_id\n",
    "        WHERE c.created_date >= '2024-01-01'\n",
    "        GROUP BY c.id, c.birth_date, c.created_date, m.id\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        age,\n",
    "        geo_type,\n",
    "        engagement_score,\n",
    "        donation_frequency,\n",
    "        COALESCE(avg_donation, 0) as avg_donation,\n",
    "        is_member\n",
    "    FROM feature_engineering\n",
    "    WHERE age BETWEEN 18 AND 90  -- Plausible Altersbereich\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data Loading & Preprocessing\n",
    "    training_data = etl_conn.execute(training_query).df()\n",
    "    \n",
    "    # Feature Engineering\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    \n",
    "    # Kategorische Variablen encodieren\n",
    "    le_geo = LabelEncoder()\n",
    "    training_data['geo_encoded'] = le_geo.fit_transform(training_data['geo_type'])\n",
    "    \n",
    "    # Features definieren\n",
    "    feature_columns = ['age', 'geo_encoded', 'engagement_score', 'donation_frequency', 'avg_donation']\n",
    "    X = training_data[feature_columns]\n",
    "    y = training_data['is_member']\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Model Training\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Model Evaluation\n",
    "    y_pred = rf_model.predict(X_test_scaled)\n",
    "    y_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    print(\"ü§ñ Propensity Model Training Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature Importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Feature Importance:\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Feature Importance\n",
    "    ax1.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    ax1.set_title('üéØ Feature Importance')\n",
    "    ax1.set_xlabel('Importance')\n",
    "    \n",
    "    # 2. Prediction Distribution\n",
    "    ax2.hist(y_pred_proba, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('üìà Propensity Score Distribution')\n",
    "    ax2.set_xlabel('Membership Probability')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax3, cmap='Blues')\n",
    "    ax3.set_title('üéØ Confusion Matrix')\n",
    "    ax3.set_xlabel('Predicted')\n",
    "    ax3.set_ylabel('Actual')\n",
    "    \n",
    "    # 4. ROC Curve\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax4.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax4.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax4.set_xlim([0.0, 1.0])\n",
    "    ax4.set_ylim([0.0, 1.05])\n",
    "    ax4.set_xlabel('False Positive Rate')\n",
    "    ax4.set_ylabel('True Positive Rate')\n",
    "    ax4.set_title('üé≠ ROC Curve')\n",
    "    ax4.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return rf_model, scaler, le_geo, feature_columns, training_data\n",
    "\n",
    "# Propensity Model trainieren\n",
    "print(\"ü§ñ Training Propensity Model...\")\n",
    "propensity_model, scaler, geo_encoder, features, training_data = create_propensity_model()\n",
    "\n",
    "# Beispiel-Vorhersage\n",
    "def predict_membership_propensity(age, geo_type, engagement_score, donation_frequency, avg_donation):\n",
    "    \"\"\"\n",
    "    Vorhersage der Mitgliedschafts-Wahrscheinlichkeit f√ºr einen Kontakt\n",
    "    \"\"\"\n",
    "    # Features vorbereiten\n",
    "    geo_encoded = geo_encoder.transform([geo_type])[0]\n",
    "    features_array = np.array([[age, geo_encoded, engagement_score, donation_frequency, avg_donation]])\n",
    "    features_scaled = scaler.transform(features_array)\n",
    "    \n",
    "    # Vorhersage\n",
    "    probability = propensity_model.predict_proba(features_scaled)[0, 1]\n",
    "    return probability\n",
    "\n",
    "# Beispiele testen\n",
    "test_cases = [\n",
    "    {\"age\": 35, \"geo_type\": \"urban\", \"engagement_score\": 65, \"donation_frequency\": 2.5, \"avg_donation\": 150},\n",
    "    {\"age\": 55, \"geo_type\": \"rural\", \"engagement_score\": 25, \"donation_frequency\": 0.8, \"avg_donation\": 75},\n",
    "    {\"age\": 28, \"geo_type\": \"suburban\", \"engagement_score\": 45, \"donation_frequency\": 1.2, \"avg_donation\": 200}\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ Propensity Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    prob = predict_membership_propensity(**case)\n",
    "    print(f\"Test Case {i}: {prob:.1%} Membership Probability\")\n",
    "    print(f\"  ‚Üí {case}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d74f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2 SEND-TIME OPTIMIZER & COPY FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "def create_send_time_optimizer():\n",
    "    \"\"\"\n",
    "    Prophet-basierter Send-Time Optimizer\n",
    "    Analysiert historische √ñffnungszeiten und optimiert Versandzeiten\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock Email Engagement Data\n",
    "    np.random.seed(42)\n",
    "    date_range = pd.date_range('2024-01-01', '2024-06-20', freq='D')\n",
    "    \n",
    "    email_data = []\n",
    "    for date in date_range:\n",
    "        for hour in range(24):\n",
    "            # Simuliere realistische √ñffnungsraten basierend auf Tageszeit\n",
    "            base_rate = 0.15  # 15% baseline\n",
    "            if 8 <= hour <= 11:  # Morning peak\n",
    "                rate_multiplier = 1.5\n",
    "            elif 14 <= hour <= 16:  # Afternoon peak\n",
    "                rate_multiplier = 1.3\n",
    "            elif 19 <= hour <= 21:  # Evening peak\n",
    "                rate_multiplier = 1.8\n",
    "            elif 0 <= hour <= 6:  # Night low\n",
    "                rate_multiplier = 0.3\n",
    "            else:\n",
    "                rate_multiplier = 1.0\n",
    "            \n",
    "            # Wochenende-Effekt\n",
    "            if date.weekday() >= 5:  # Weekend\n",
    "                rate_multiplier *= 0.7\n",
    "            \n",
    "            open_rate = base_rate * rate_multiplier + np.random.normal(0, 0.02)\n",
    "            open_rate = max(0, min(1, open_rate))  # Clamp zwischen 0 und 1\n",
    "            \n",
    "            email_data.append({\n",
    "                'date': date,\n",
    "                'hour': hour,\n",
    "                'datetime': date + pd.Timedelta(hours=hour),\n",
    "                'open_rate': open_rate,\n",
    "                'sent_count': np.random.randint(50, 200),\n",
    "                'opens': int(open_rate * np.random.randint(50, 200))\n",
    "            })\n",
    "    \n",
    "    email_df = pd.DataFrame(email_data)\n",
    "    \n",
    "    # Prophet Model f√ºr jede Stunde\n",
    "    hourly_models = {}\n",
    "    optimal_hours = {}\n",
    "    \n",
    "    print(\"‚è∞ Training Send-Time Optimization Models...\")\n",
    "    \n",
    "    for hour in range(24):\n",
    "        hour_data = email_df[email_df['hour'] == hour].copy()\n",
    "        \n",
    "        # Prophet format\n",
    "        prophet_data = hour_data[['date', 'open_rate']].rename(columns={'date': 'ds', 'open_rate': 'y'})\n",
    "        \n",
    "        # Prophet model\n",
    "        model = Prophet(\n",
    "            daily_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=False,\n",
    "            interval_width=0.95\n",
    "        )\n",
    "        \n",
    "        model.fit(prophet_data)\n",
    "        hourly_models[hour] = model\n",
    "        \n",
    "        # Durchschnittliche Performance f√ºr diese Stunde\n",
    "        avg_performance = hour_data['open_rate'].mean()\n",
    "        optimal_hours[hour] = avg_performance\n",
    "    \n",
    "    # Beste Stunden identifizieren\n",
    "    best_hours = sorted(optimal_hours.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Hourly Performance Heatmap\n",
    "    hourly_avg = email_df.groupby('hour')['open_rate'].mean()\n",
    "    ax1.bar(range(24), hourly_avg, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('üìß Durchschnittliche √ñffnungsrate pro Stunde')\n",
    "    ax1.set_xlabel('Stunde')\n",
    "    ax1.set_ylabel('√ñffnungsrate')\n",
    "    ax1.set_xticks(range(0, 24, 2))\n",
    "    \n",
    "    # 2. Weekly Pattern\n",
    "    email_df['weekday'] = email_df['date'].dt.day_name()\n",
    "    weekly_avg = email_df.groupby('weekday')['open_rate'].mean()\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekly_avg = weekly_avg.reindex(weekday_order)\n",
    "    ax2.plot(weekly_avg.values, marker='o', color='orange')\n",
    "    ax2.set_title('üìÖ W√∂chentliches √ñffnungsverhalten')\n",
    "    ax2.set_xticks(range(7))\n",
    "    ax2.set_xticklabels([day[:3] for day in weekday_order])\n",
    "    ax2.set_ylabel('√ñffnungsrate')\n",
    "    \n",
    "    # 3. Heatmap: Hour vs Weekday\n",
    "    heatmap_data = email_df.pivot_table(index='hour', columns=email_df['date'].dt.day_name(), values='open_rate', aggfunc='mean')\n",
    "    heatmap_data = heatmap_data.reindex(columns=weekday_order)\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', ax=ax3, cbar_kws={'label': '√ñffnungsrate'})\n",
    "    ax3.set_title('üî• √ñffnungsraten Heatmap')\n",
    "    ax3.set_xlabel('Wochentag')\n",
    "    ax3.set_ylabel('Stunde')\n",
    "    \n",
    "    # 4. Top Performing Hours\n",
    "    top_hours, top_rates = zip(*best_hours)\n",
    "    ax4.bar(range(len(top_hours)), top_rates, color='green', alpha=0.7)\n",
    "    ax4.set_title('üèÜ Top 5 Versandzeiten')\n",
    "    ax4.set_xticks(range(len(top_hours)))\n",
    "    ax4.set_xticklabels([f'{h}:00' for h in top_hours])\n",
    "    ax4.set_ylabel('Durchschnittliche √ñffnungsrate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ Optimale Versandzeiten:\")\n",
    "    for hour, rate in best_hours:\n",
    "        print(f\"   {hour:2d}:00 Uhr - {rate:.1%} √ñffnungsrate\")\n",
    "    \n",
    "    return hourly_models, optimal_hours, email_df\n",
    "\n",
    "def demo_copy_fine_tuning():\n",
    "    \"\"\"\n",
    "    Demo f√ºr OpenAI Copy Fine-Tuning\n",
    "    Optimiert E-Mail-Betreffs basierend auf historischen √ñffnungsraten\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock Training Data f√ºr E-Mail-Betreffs\n",
    "    email_subjects_data = [\n",
    "        {\"subject\": \"Ihre Spende macht einen Unterschied\", \"open_rate\": 0.24, \"category\": \"gratitude\"},\n",
    "        {\"subject\": \"Dringend: Hilfe f√ºr Bed√ºrftige\", \"open_rate\": 0.31, \"category\": \"urgency\"},\n",
    "        {\"subject\": \"Werden Sie Teil unserer Mission\", \"open_rate\": 0.19, \"category\": \"invitation\"},\n",
    "        {\"subject\": \"Monatlicher Newsletter - Juni 2025\", \"open_rate\": 0.15, \"category\": \"newsletter\"},\n",
    "        {\"subject\": \"üéØ Nur noch 3 Tage: Verdoppeln Sie Ihre Wirkung\", \"open_rate\": 0.28, \"category\": \"urgency\"},\n",
    "        {\"subject\": \"Herzlichen Dank f√ºr Ihre Unterst√ºtzung\", \"open_rate\": 0.22, \"category\": \"gratitude\"},\n",
    "        {\"subject\": \"Neu: Online Mitgliederportal ist da!\", \"open_rate\": 0.26, \"category\": \"announcement\"},\n",
    "        {\"subject\": \"Ihre Hilfe wird dringend ben√∂tigt\", \"open_rate\": 0.29, \"category\": \"urgency\"},\n",
    "        {\"subject\": \"Einladung zur Jahreshauptversammlung\", \"open_rate\": 0.18, \"category\": \"invitation\"},\n",
    "        {\"subject\": \"üíù Ein besonderer Dank an Sie\", \"open_rate\": 0.25, \"category\": \"gratitude\"}\n",
    "    ]\n",
    "    \n",
    "    subjects_df = pd.DataFrame(email_subjects_data)\n",
    "    \n",
    "    # Analyse der Performance nach Kategorien\n",
    "    category_performance = subjects_df.groupby('category').agg({\n",
    "        'open_rate': ['mean', 'count', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"‚úçÔ∏è E-Mail-Betreff Performance Analyse:\")\n",
    "    print(\"=\"*50)\n",
    "    print(category_performance)\n",
    "    \n",
    "    # Best Practices ableiten\n",
    "    best_subjects = subjects_df.nlargest(3, 'open_rate')\n",
    "    print(f\"\\nüèÜ Top Performing Subjects:\")\n",
    "    for _, row in best_subjects.iterrows():\n",
    "        print(f\"   {row['open_rate']:.1%} - \\\"{row['subject']}\\\" ({row['category']})\")\n",
    "    \n",
    "    # OpenAI Fine-Tuning Setup (Demo)\n",
    "    fine_tuning_prompt = f\"\"\"\n",
    "    # OpenAI Fine-Tuning f√ºr E-Mail-Betreffs\n",
    "    \n",
    "    ## Training Data Format:\n",
    "    ```json\n",
    "    {{\n",
    "        \"prompt\": \"Erstelle einen E-Mail-Betreff f√ºr Kategorie: {category}\",\n",
    "        \"completion\": \"Optimierter Betreff basierend auf {open_rate:.1%} Performance\"\n",
    "    }}\n",
    "    ```\n",
    "    \n",
    "    ## Beste Praktiken:\n",
    "    - Urgency-Betreffs: {subjects_df[subjects_df['category']=='urgency']['open_rate'].mean():.1%} avg. open rate\n",
    "    - Gratitude-Betreffs: {subjects_df[subjects_df['category']=='gratitude']['open_rate'].mean():.1%} avg. open rate\n",
    "    - Emojis erh√∂hen Engagement um ~15%\n",
    "    - Zeitliche Verknappung funktioniert gut\n",
    "    \n",
    "    ## Implementation:\n",
    "    1. Sammle historische Daten via CiviCRM MailingSummary.get\n",
    "    2. Erstelle Fine-Tuning Dataset\n",
    "    3. Trainiere via OpenAI API\n",
    "    4. Integriere in n8n Template-Generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{fine_tuning_prompt}\")\n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Subject Performance\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = {'urgency': 'red', 'gratitude': 'green', 'invitation': 'blue', \n",
    "              'newsletter': 'gray', 'announcement': 'orange'}\n",
    "    for category in subjects_df['category'].unique():\n",
    "        cat_data = subjects_df[subjects_df['category'] == category]\n",
    "        plt.scatter(range(len(cat_data)), cat_data['open_rate'], \n",
    "                   color=colors.get(category, 'black'), label=category, s=100, alpha=0.7)\n",
    "    \n",
    "    plt.title('üìä Betreff-Performance nach Kategorie')\n",
    "    plt.xlabel('E-Mail Index')\n",
    "    plt.ylabel('√ñffnungsrate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Category Averages\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cat_avg = subjects_df.groupby('category')['open_rate'].mean().sort_values(ascending=False)\n",
    "    bars = plt.bar(range(len(cat_avg)), cat_avg.values, \n",
    "                   color=[colors.get(cat, 'gray') for cat in cat_avg.index], alpha=0.7)\n",
    "    plt.title('üìà Durchschnittliche Performance pro Kategorie')\n",
    "    plt.xticks(range(len(cat_avg)), cat_avg.index, rotation=45)\n",
    "    plt.ylabel('√ñffnungsrate')\n",
    "    \n",
    "    # Werte auf Balken anzeigen\n",
    "    for bar, value in zip(bars, cat_avg.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{value:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return subjects_df\n",
    "\n",
    "# Send-Time Optimizer ausf√ºhren\n",
    "print(\"‚è∞ Creating Send-Time Optimizer...\")\n",
    "time_models, optimal_times, email_engagement_data = create_send_time_optimizer()\n",
    "\n",
    "print(\"\\n‚úçÔ∏è Analyzing Copy Performance...\")\n",
    "copy_analysis = demo_copy_fine_tuning()\n",
    "\n",
    "print(f\"\\nü§ñ KI-Personalisierung Setup Complete:\")\n",
    "print(f\"   ‚úÖ Propensity Model trained with {len(training_data)} samples\")\n",
    "print(f\"   ‚úÖ Send-Time Optimizer covering 24 hours\")\n",
    "print(f\"   ‚úÖ Copy Analysis with {len(copy_analysis)} subject variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556317a9",
   "metadata": {},
   "source": [
    "# 4. Volunteer-Lifecycle (F-19 bis F-22)\n",
    "\n",
    "## üë• Erweiterte n8n Workflows f√ºr Ehrenamt-Management\n",
    "\n",
    "Phase III erweitert die Plattform um professionelles Volunteer-Management mit 4 neuen Workflows:\n",
    "\n",
    "| Workflow | Trigger | Aktion | Ergebnis |\n",
    "|----------|---------|---------|----------|\n",
    "| **F-19_vol_app** | Webform ¬ªFreiwillig mithelfen¬´ | Contact tag = *Volunteer-Lead*; Mail ‚ÄûErstinfo\" | Lead-Pipeline |\n",
    "| **F-20_skills_match** | Tag = Volunteer-Lead & Klick ¬ªPortfolio-Link¬´ | n8n ‚Üí Airtable Skills-DB | Skill-Profil |\n",
    "| **F-21_assignment** | Staff ordnet Task in Airtable zu | n8n ‚Üí Create Activity *Volunteer Assignment* | Portal-Todo & Slack DM |\n",
    "| **F-22_thank_you** | Activity.type = *Assignment* & Status = Done | Email.send ‚ÄûDank & Nachweis\" + OpenBadges-JSON | Bindet Ehrenamt in Gesamt-CRM ein |\n",
    "\n",
    "### üéØ Integration Points:\n",
    "- **Airtable Skills-DB**: Zentrale Skills-Matrix f√ºr Matching\n",
    "- **OpenBadges**: Digitale Anerkennungs-Zertifikate  \n",
    "- **Slack Integration**: Real-time Team-Kommunikation\n",
    "- **CiviCRM Activities**: Vollst√§ndige Volunteer-History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.1 VOLUNTEER WORKFLOWS - F-19 bis F-22 Implementation\n",
    "# =============================================================================\n",
    "\n",
    "def create_volunteer_workflows():\n",
    "    \"\"\"\n",
    "    Volunteer-Lifecycle Workflows f√ºr erweiterte Ehrenamt-Verwaltung\n",
    "    Integration: CiviCRM ‚Üí n8n ‚Üí Airtable ‚Üí OpenBadges\n",
    "    \"\"\"\n",
    "    \n",
    "    # F-19: Volunteer Application Workflow\n",
    "    f19_workflow = {\n",
    "        \"name\": \"F-19_volunteer_application\",\n",
    "        \"description\": \"Freiwilligen-Anmeldung verarbeiten\",\n",
    "        \"trigger\": {\n",
    "            \"type\": \"webhook\",\n",
    "            \"url\": \"/webhook/volunteer-application\",\n",
    "            \"method\": \"POST\"\n",
    "        },\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"Parse Application\",\n",
    "                \"type\": \"function\",\n",
    "                \"code\": \"\"\"\n",
    "                // Volunteer Application Parser\n",
    "                const applicationData = items[0].json;\n",
    "                \n",
    "                return [{\n",
    "                    contact_id: applicationData.contact_id,\n",
    "                    email: applicationData.email,\n",
    "                    skills: applicationData.skills || [],\n",
    "                    availability: applicationData.availability || 'flexible',\n",
    "                    motivation: applicationData.motivation || '',\n",
    "                    timestamp: new Date().toISOString()\n",
    "                }];\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"CiviCRM Tag Contact\",\n",
    "                \"type\": \"civicrm\",\n",
    "                \"operation\": \"contact.tag\",\n",
    "                \"parameters\": {\n",
    "                    \"contact_id\": \"={{$json.contact_id}}\",\n",
    "                    \"tag\": \"Volunteer-Lead\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Send Welcome Email\",\n",
    "                \"type\": \"email\",\n",
    "                \"parameters\": {\n",
    "                    \"to\": \"={{$json.email}}\",\n",
    "                    \"subject\": \"Willkommen im Volunteer-Team!\",\n",
    "                    \"template\": \"volunteer_welcome\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # F-20: Skills Matching Workflow  \n",
    "    f20_workflow = {\n",
    "        \"name\": \"F-20_skills_matching\",\n",
    "        \"description\": \"Skills-Profil in Airtable erstellen\",\n",
    "        \"trigger\": {\n",
    "            \"type\": \"webhook\", \n",
    "            \"url\": \"/webhook/skills-portfolio-click\"\n",
    "        },\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"Get Contact Data\",\n",
    "                \"type\": \"civicrm\",\n",
    "                \"operation\": \"contact.get\",\n",
    "                \"parameters\": {\n",
    "                    \"contact_id\": \"={{$json.contact_id}}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Create Skills Profile\",\n",
    "                \"type\": \"airtable\",\n",
    "                \"operation\": \"create\",\n",
    "                \"parameters\": {\n",
    "                    \"base\": AIRTABLE_BASE_ID,\n",
    "                    \"table\": \"volunteer_skills\",\n",
    "                    \"fields\": {\n",
    "                        \"Contact_ID\": \"={{$json.contact_id}}\",\n",
    "                        \"Name\": \"={{$json.display_name}}\",\n",
    "                        \"Email\": \"={{$json.email}}\",\n",
    "                        \"Skills\": \"={{$json.skills}}\",\n",
    "                        \"Availability\": \"={{$json.availability}}\",\n",
    "                        \"Status\": \"Active\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # F-21: Task Assignment Workflow\n",
    "    f21_workflow = {\n",
    "        \"name\": \"F-21_task_assignment\", \n",
    "        \"description\": \"Aufgaben-Zuweisung und Benachrichtigung\",\n",
    "        \"trigger\": {\n",
    "            \"type\": \"airtable_webhook\",\n",
    "            \"table\": \"volunteer_tasks\",\n",
    "            \"event\": \"record_updated\"\n",
    "        },\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"Check Assignment\",\n",
    "                \"type\": \"function\",\n",
    "                \"code\": \"\"\"\n",
    "                const record = items[0].json;\n",
    "                \n",
    "                if (record.fields.Assigned_Volunteer && !record.fields.Notified) {\n",
    "                    return [{\n",
    "                        volunteer_id: record.fields.Assigned_Volunteer,\n",
    "                        task_title: record.fields.Task_Title,\n",
    "                        task_description: record.fields.Description,\n",
    "                        due_date: record.fields.Due_Date,\n",
    "                        record_id: record.id\n",
    "                    }];\n",
    "                }\n",
    "                return [];\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Create CiviCRM Activity\",\n",
    "                \"type\": \"civicrm\",\n",
    "                \"operation\": \"activity.create\",\n",
    "                \"parameters\": {\n",
    "                    \"activity_type\": \"Volunteer Assignment\",\n",
    "                    \"source_contact_id\": \"={{$json.volunteer_id}}\",\n",
    "                    \"subject\": \"={{$json.task_title}}\",\n",
    "                    \"details\": \"={{$json.task_description}}\",\n",
    "                    \"activity_date_time\": \"now\",\n",
    "                    \"status_id\": \"Scheduled\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Send Slack Notification\",\n",
    "                \"type\": \"slack\",\n",
    "                \"parameters\": {\n",
    "                    \"channel\": \"#volunteers\",\n",
    "                    \"message\": \"üéØ Neue Aufgabe zugewiesen: {{$json.task_title}}\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # F-22: Thank You & Badge Workflow\n",
    "    f22_workflow = {\n",
    "        \"name\": \"F-22_thank_you_badge\",\n",
    "        \"description\": \"Dankesch√∂n und OpenBadges bei Aufgaben-Abschluss\",\n",
    "        \"trigger\": {\n",
    "            \"type\": \"civicrm_activity_webhook\",\n",
    "            \"activity_type\": \"Volunteer Assignment\",\n",
    "            \"status\": \"Completed\"\n",
    "        },\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"Generate OpenBadge\",\n",
    "                \"type\": \"function\", \n",
    "                \"code\": \"\"\"\n",
    "                const activity = items[0].json;\n",
    "                \n",
    "                const badge = {\n",
    "                    \"@context\": \"https://w3id.org/openbadges/v2\",\n",
    "                    \"type\": \"Assertion\",\n",
    "                    \"id\": `https://menschlichkeit.at/badges/${activity.id}`,\n",
    "                    \"recipient\": {\n",
    "                        \"type\": \"email\",\n",
    "                        \"hashed\": false,\n",
    "                        \"identity\": activity.contact_email\n",
    "                    },\n",
    "                    \"badge\": {\n",
    "                        \"type\": \"BadgeClass\",\n",
    "                        \"id\": \"https://menschlichkeit.at/badges/volunteer-helper\",\n",
    "                        \"name\": \"Volunteer Helper\",\n",
    "                        \"description\": \"Ausgezeichnet f√ºr ehrenamtliche Mitarbeit\",\n",
    "                        \"image\": \"https://menschlichkeit.at/badges/volunteer-helper.png\",\n",
    "                        \"criteria\": \"Erfolgreiche Durchf√ºhrung einer freiwilligen Aufgabe\",\n",
    "                        \"issuer\": {\n",
    "                            \"type\": \"Profile\", \n",
    "                            \"id\": \"https://menschlichkeit.at\",\n",
    "                            \"name\": \"Menschlichkeit √ñsterreich\",\n",
    "                            \"email\": \"badges@menschlichkeit.at\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"issuedOn\": new Date().toISOString(),\n",
    "                    \"verification\": {\n",
    "                        \"type\": \"hosted\"\n",
    "                    }\n",
    "                };\n",
    "                \n",
    "                return [{ badge: badge, activity: activity }];\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Send Thank You Email\",\n",
    "                \"type\": \"email\",\n",
    "                \"parameters\": {\n",
    "                    \"to\": \"={{$json.activity.contact_email}}\",\n",
    "                    \"subject\": \"Herzlichen Dank f√ºr Ihre Hilfe! üèÜ\",\n",
    "                    \"template\": \"volunteer_thank_you\",\n",
    "                    \"attachments\": [{\n",
    "                        \"filename\": \"volunteer_badge.json\",\n",
    "                        \"content\": \"={{JSON.stringify($json.badge)}}\"\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    workflows = [f19_workflow, f20_workflow, f21_workflow, f22_workflow]\n",
    "    \n",
    "    print(\"üë• Volunteer Workflows Created:\")\n",
    "    print(\"=\"*50)\n",
    "    for workflow in workflows:\n",
    "        print(f\"‚úÖ {workflow['name']}: {workflow['description']}\")\n",
    "    \n",
    "    return workflows\n",
    "\n",
    "def demo_airtable_skills_db():\n",
    "    \"\"\"\n",
    "    Demo der Airtable Skills-Datenbank f√ºr Volunteer-Matching\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock Skills Database Schema\n",
    "    skills_db_schema = {\n",
    "        \"volunteer_skills\": {\n",
    "            \"fields\": [\n",
    "                {\"name\": \"Contact_ID\", \"type\": \"number\"},\n",
    "                {\"name\": \"Name\", \"type\": \"singleLineText\"},\n",
    "                {\"name\": \"Email\", \"type\": \"email\"},\n",
    "                {\"name\": \"Skills\", \"type\": \"multipleSelect\", \"options\": [\n",
    "                    \"IT/Web\", \"Grafik/Design\", \"Texten/PR\", \"Event-Orga\", \n",
    "                    \"Fundraising\", \"Rechtliches\", \"√úbersetzung\", \"Fotografie\"\n",
    "                ]},\n",
    "                {\"name\": \"Availability\", \"type\": \"singleSelect\", \"options\": [\n",
    "                    \"Wochenende\", \"Abends\", \"Flexibel\", \"Nur Events\"\n",
    "                ]},\n",
    "                {\"name\": \"Experience_Level\", \"type\": \"singleSelect\", \"options\": [\n",
    "                    \"Anf√§nger\", \"Fortgeschritten\", \"Expert\"\n",
    "                ]},\n",
    "                {\"name\": \"Status\", \"type\": \"singleSelect\", \"options\": [\n",
    "                    \"Active\", \"Inactive\", \"On Hold\"\n",
    "                ]}\n",
    "            ]\n",
    "        },\n",
    "        \"volunteer_tasks\": {\n",
    "            \"fields\": [\n",
    "                {\"name\": \"Task_Title\", \"type\": \"singleLineText\"},\n",
    "                {\"name\": \"Description\", \"type\": \"longText\"},\n",
    "                {\"name\": \"Required_Skills\", \"type\": \"multipleSelect\"},\n",
    "                {\"name\": \"Assigned_Volunteer\", \"type\": \"singleLineText\"},\n",
    "                {\"name\": \"Due_Date\", \"type\": \"date\"},\n",
    "                {\"name\": \"Status\", \"type\": \"singleSelect\", \"options\": [\n",
    "                    \"Open\", \"Assigned\", \"In Progress\", \"Completed\"\n",
    "                ]},\n",
    "                {\"name\": \"Notified\", \"type\": \"checkbox\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Mock Volunteer Data\n",
    "    mock_volunteers = [\n",
    "        {\n",
    "            \"Contact_ID\": 1001,\n",
    "            \"Name\": \"Anna Mueller\",\n",
    "            \"Email\": \"anna.mueller@example.com\",\n",
    "            \"Skills\": [\"IT/Web\", \"Grafik/Design\"],\n",
    "            \"Availability\": \"Abends\",\n",
    "            \"Experience_Level\": \"Fortgeschritten\",\n",
    "            \"Status\": \"Active\"\n",
    "        },\n",
    "        {\n",
    "            \"Contact_ID\": 1002, \n",
    "            \"Name\": \"Max Weber\",\n",
    "            \"Email\": \"max.weber@example.com\",\n",
    "            \"Skills\": [\"Event-Orga\", \"Fundraising\"],\n",
    "            \"Availability\": \"Wochenende\",\n",
    "            \"Experience_Level\": \"Expert\",\n",
    "            \"Status\": \"Active\"\n",
    "        },\n",
    "        {\n",
    "            \"Contact_ID\": 1003,\n",
    "            \"Name\": \"Lisa Schmidt\", \n",
    "            \"Email\": \"lisa.schmidt@example.com\",\n",
    "            \"Skills\": [\"Texten/PR\", \"√úbersetzung\"],\n",
    "            \"Availability\": \"Flexibel\",\n",
    "            \"Experience_Level\": \"Fortgeschritten\", \n",
    "            \"Status\": \"Active\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    mock_tasks = [\n",
    "        {\n",
    "            \"Task_Title\": \"Website-Update f√ºr Kampagne\",\n",
    "            \"Description\": \"Neue Spendenkampagne auf der Website einbinden\",\n",
    "            \"Required_Skills\": [\"IT/Web\", \"Grafik/Design\"],\n",
    "            \"Assigned_Volunteer\": \"Anna Mueller\",\n",
    "            \"Due_Date\": \"2025-07-15\",\n",
    "            \"Status\": \"Assigned\",\n",
    "            \"Notified\": True\n",
    "        },\n",
    "        {\n",
    "            \"Task_Title\": \"Charity-Event Organisation\",\n",
    "            \"Description\": \"Sommerfest f√ºr Spender und Mitglieder organisieren\",\n",
    "            \"Required_Skills\": [\"Event-Orga\"],\n",
    "            \"Assigned_Volunteer\": \"Max Weber\", \n",
    "            \"Due_Date\": \"2025-08-20\",\n",
    "            \"Status\": \"In Progress\",\n",
    "            \"Notified\": True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    volunteers_df = pd.DataFrame(mock_volunteers)\n",
    "    tasks_df = pd.DataFrame(mock_tasks)\n",
    "    \n",
    "    # Skills Analysis\n",
    "    all_skills = []\n",
    "    for skills_list in volunteers_df['Skills']:\n",
    "        all_skills.extend(skills_list)\n",
    "    \n",
    "    skills_count = pd.Series(all_skills).value_counts()\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Skills Distribution\n",
    "    ax1.bar(skills_count.index, skills_count.values, color='lightblue', alpha=0.7)\n",
    "    ax1.set_title('üéØ Verf√ºgbare Skills im Volunteer-Pool')\n",
    "    ax1.set_ylabel('Anzahl Volunteers')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Availability Pattern\n",
    "    availability_count = volunteers_df['Availability'].value_counts()\n",
    "    ax2.pie(availability_count.values, labels=availability_count.index, autopct='%1.1f%%')\n",
    "    ax2.set_title('‚è∞ Verf√ºgbarkeits-Verteilung')\n",
    "    \n",
    "    # 3. Experience Levels\n",
    "    experience_count = volunteers_df['Experience_Level'].value_counts()\n",
    "    colors = {'Anf√§nger': 'lightcoral', 'Fortgeschritten': 'orange', 'Expert': 'green'}\n",
    "    ax3.bar(experience_count.index, experience_count.values, \n",
    "            color=[colors.get(x, 'gray') for x in experience_count.index], alpha=0.7)\n",
    "    ax3.set_title('üìà Erfahrungs-Level der Volunteers')\n",
    "    ax3.set_ylabel('Anzahl')\n",
    "    \n",
    "    # 4. Task Status\n",
    "    task_status = tasks_df['Status'].value_counts()\n",
    "    ax4.pie(task_status.values, labels=task_status.index, autopct='%1.1f%%')\n",
    "    ax4.set_title('üìã Task Status Overview')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üë• Volunteer Skills Database:\")\n",
    "    print(\"=\"*50)\n",
    "    print(volunteers_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìã Current Tasks:\")\n",
    "    print(\"=\"*50)\n",
    "    print(tasks_df.to_string(index=False))\n",
    "    \n",
    "    return volunteers_df, tasks_df, skills_db_schema\n",
    "\n",
    "# Volunteer Workflows erstellen\n",
    "print(\"üë• Creating Volunteer-Lifecycle Workflows...\")\n",
    "volunteer_workflows = create_volunteer_workflows()\n",
    "\n",
    "print(\"\\nüìä Setting up Airtable Skills Database...\")\n",
    "volunteers_data, tasks_data, airtable_schema = demo_airtable_skills_db()\n",
    "\n",
    "print(f\"\\nüéØ Volunteer Management Setup Complete:\")\n",
    "print(f\"   ‚úÖ {len(volunteer_workflows)} n8n Workflows (F-19 bis F-22)\")\n",
    "print(f\"   ‚úÖ Skills Database mit {len(volunteers_data)} aktiven Volunteers\")\n",
    "print(f\"   ‚úÖ {len(tasks_data)} Tasks im System\")\n",
    "print(f\"   ‚úÖ OpenBadges Integration f√ºr Anerkennungen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3be809",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è 5. Governance, Risk & Compliance Automatisierung\n",
    "\n",
    "### Automatisierte Security-Scans, Backup-Strategien und DSAR-Processing\n",
    "\n",
    "Diese Sektion implementiert die Automatisierung f√ºr Governance, Risk & Compliance (GRC) Anforderungen, einschlie√ülich:\n",
    "- **Kubernetes Security Scans** mit kube-bench\n",
    "- **Automatisierte Backup-Strategien** mit Velero\n",
    "- **PR-Template f√ºr Compliance-Checks**\n",
    "- **DSAR (Data Subject Access Request) n8n Workflow**\n",
    "- **Compliance-Dashboard und -Metriken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.1 Kubernetes Security Scans mit kube-bench\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import yaml\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def run_kube_bench():\n",
    "    \"\"\"\n",
    "    Automatisierte Kubernetes Security Scans mit kube-bench\n",
    "    \"\"\"\n",
    "    print(\"üîç Starte Kubernetes Security Scan mit kube-bench...\")\n",
    "    \n",
    "    # kube-bench Konfiguration\n",
    "    kube_bench_config = {\n",
    "        \"version\": \"cis-1.8\",\n",
    "        \"node_type\": \"master\",\n",
    "        \"output_format\": \"json\",\n",
    "        \"check_groups\": [\n",
    "            \"master\",\n",
    "            \"etcd\", \n",
    "            \"controlplane\",\n",
    "            \"node\",\n",
    "            \"policies\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Simuliere kube-bench Ausgabe (in echter Implementierung w√ºrde kubectl/kube-bench verwendet)\n",
    "    mock_results = {\n",
    "        \"Controls\": [\n",
    "            {\n",
    "                \"id\": \"1.1.1\",\n",
    "                \"text\": \"Ensure that the API server pod specification file permissions are set to 644 or more restrictive\",\n",
    "                \"state\": \"PASS\",\n",
    "                \"scored\": True,\n",
    "                \"remediation\": \"Run the below command on the master node: chmod 644 /etc/kubernetes/manifests/kube-apiserver.yaml\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"1.2.1\", \n",
    "                \"text\": \"Ensure that the --anonymous-auth argument is set to false\",\n",
    "                \"state\": \"FAIL\",\n",
    "                \"scored\": True,\n",
    "                \"remediation\": \"Edit the API server pod specification file and set --anonymous-auth=false\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"4.1.1\",\n",
    "                \"text\": \"Ensure that the kubelet service file permissions are set to 644 or more restrictive\",\n",
    "                \"state\": \"WARN\",\n",
    "                \"scored\": False,\n",
    "                \"remediation\": \"Run chmod 644 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\"\n",
    "            }\n",
    "        ],\n",
    "        \"Totals\": {\n",
    "            \"total_pass\": 95,\n",
    "            \"total_fail\": 8,\n",
    "            \"total_warn\": 12,\n",
    "            \"total_info\": 0\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"passed\": 95,\n",
    "            \"failed\": 8,\n",
    "            \"warned\": 12,\n",
    "            \"total\": 115\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generiere Security Report\n",
    "    security_report = generate_security_report(mock_results)\n",
    "    \n",
    "    # Speichere Report\n",
    "    save_security_report(security_report)\n",
    "    \n",
    "    return security_report\n",
    "\n",
    "def generate_security_report(results):\n",
    "    \"\"\"\n",
    "    Generiert detaillierten Security Report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"scan_type\": \"kube-bench\",\n",
    "        \"summary\": results[\"summary\"],\n",
    "        \"critical_findings\": [],\n",
    "        \"recommendations\": [],\n",
    "        \"compliance_score\": 0\n",
    "    }\n",
    "    \n",
    "    # Berechne Compliance Score\n",
    "    total_checks = results[\"summary\"][\"total\"]\n",
    "    passed_checks = results[\"summary\"][\"passed\"]\n",
    "    report[\"compliance_score\"] = round((passed_checks / total_checks) * 100, 2)\n",
    "    \n",
    "    # Extrahiere kritische Findings\n",
    "    for control in results[\"Controls\"]:\n",
    "        if control[\"state\"] == \"FAIL\" and control[\"scored\"]:\n",
    "            report[\"critical_findings\"].append({\n",
    "                \"id\": control[\"id\"],\n",
    "                \"description\": control[\"text\"],\n",
    "                \"remediation\": control[\"remediation\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            })\n",
    "    \n",
    "    # Generiere Empfehlungen\n",
    "    if report[\"compliance_score\"] < 90:\n",
    "        report[\"recommendations\"].append(\"Immediate action required: Compliance score below 90%\")\n",
    "    if len(report[\"critical_findings\"]) > 5:\n",
    "        report[\"recommendations\"].append(\"Critical: More than 5 high-priority security issues found\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "def save_security_report(report):\n",
    "    \"\"\"\n",
    "    Speichert Security Report f√ºr weitere Verarbeitung\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"security_scan_{timestamp}.json\"\n",
    "    \n",
    "    # In echter Implementierung w√ºrde hier in MinIO/S3 gespeichert\n",
    "    print(f\"üíæ Security Report gespeichert: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# F√ºhre Security Scan aus\n",
    "security_report = run_kube_bench()\n",
    "\n",
    "print(\"üõ°Ô∏è Kubernetes Security Scan abgeschlossen!\")\n",
    "print(f\"üìä Compliance Score: {security_report['compliance_score']}%\")\n",
    "print(f\"üîç Kritische Findings: {len(security_report['critical_findings'])}\")\n",
    "print(f\"üí° Empfehlungen: {len(security_report['recommendations'])}\")\n",
    "\n",
    "# Visualisiere Security Metrics\n",
    "compliance_data = {\n",
    "    'Passed': security_report['summary']['passed'],\n",
    "    'Failed': security_report['summary']['failed'], \n",
    "    'Warnings': security_report['summary']['warned']\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie Chart f√ºr Security Status\n",
    "ax1.pie(compliance_data.values(), labels=compliance_data.keys(), autopct='%1.1f%%', \n",
    "        colors=['#28a745', '#dc3545', '#ffc107'])\n",
    "ax1.set_title('Kubernetes Security Compliance Status')\n",
    "\n",
    "# Bar Chart f√ºr Compliance Score Entwicklung (simuliert)\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'Mai', 'Jun']\n",
    "scores = [82, 85, 87, 89, 91, security_report['compliance_score']]\n",
    "\n",
    "ax2.bar(months, scores, color=['#17a2b8' if score >= 90 else '#ffc107' for score in scores])\n",
    "ax2.set_title('Compliance Score Entwicklung')\n",
    "ax2.set_ylabel('Compliance Score (%)')\n",
    "ax2.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='Minimum Threshold (90%)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Kritische Security Findings:\")\n",
    "for finding in security_report['critical_findings'][:3]:  # Zeige nur Top 3\n",
    "    print(f\"  ‚Ä¢ {finding['id']}: {finding['description'][:60]}...\")\n",
    "    print(f\"    üí° L√∂sung: {finding['remediation'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.2 Automatisierte Backup-Strategien mit Velero\n",
    "# =============================================================================\n",
    "\n",
    "def setup_velero_backup_strategy():\n",
    "    \"\"\"\n",
    "    Konfiguriert automatisierte Backup-Strategien mit Velero\n",
    "    \"\"\"\n",
    "    print(\"üíæ Konfiguriere Velero Backup-Strategien...\")\n",
    "    \n",
    "    # Velero Backup Konfiguration\n",
    "    backup_schedules = {\n",
    "        \"daily-backup\": {\n",
    "            \"schedule\": \"0 2 * * *\",  # T√§glich um 2 Uhr\n",
    "            \"include_namespaces\": [\"n8n-production\", \"civicrm\"],\n",
    "            \"retention\": \"30d\",\n",
    "            \"storage_location\": \"minio-backup\"\n",
    "        },\n",
    "        \"weekly-full-backup\": {\n",
    "            \"schedule\": \"0 1 * * 0\",  # Sonntags um 1 Uhr\n",
    "            \"include_namespaces\": [\"*\"],\n",
    "            \"retention\": \"12w\", \n",
    "            \"storage_location\": \"minio-backup\"\n",
    "        },\n",
    "        \"pre-deployment-backup\": {\n",
    "            \"trigger\": \"manual\",\n",
    "            \"include_namespaces\": [\"n8n-production\"],\n",
    "            \"retention\": \"7d\",\n",
    "            \"storage_location\": \"minio-backup\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generiere Velero YAML Konfigurationen\n",
    "    for name, config in backup_schedules.items():\n",
    "        velero_config = generate_velero_schedule(name, config)\n",
    "        print(f\"‚úÖ Backup Schedule '{name}' konfiguriert\")\n",
    "        print(f\"   üìÖ Schedule: {config.get('schedule', 'Manual')}\")\n",
    "        print(f\"   üóÇÔ∏è Namespaces: {', '.join(config['include_namespaces'])}\")\n",
    "        print(f\"   ‚è≥ Retention: {config['retention']}\")\n",
    "        print()\n",
    "    \n",
    "    return backup_schedules\n",
    "\n",
    "def generate_velero_schedule(name, config):\n",
    "    \"\"\"\n",
    "    Generiert Velero Schedule YAML\n",
    "    \"\"\"\n",
    "    velero_schedule = {\n",
    "        \"apiVersion\": \"velero.io/v1\",\n",
    "        \"kind\": \"Schedule\",\n",
    "        \"metadata\": {\n",
    "            \"name\": name,\n",
    "            \"namespace\": \"velero\"\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"schedule\": config.get(\"schedule\", \"\"),\n",
    "            \"template\": {\n",
    "                \"includedNamespaces\": config[\"include_namespaces\"],\n",
    "                \"storageLocation\": config[\"storage_location\"],\n",
    "                \"ttl\": config[\"retention\"],\n",
    "                \"defaultVolumesToRestic\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return velero_schedule\n",
    "\n",
    "# =============================================================================\n",
    "# 5.3 DSAR (Data Subject Access Request) n8n Workflow\n",
    "# =============================================================================\n",
    "\n",
    "def create_dsar_workflow():\n",
    "    \"\"\"\n",
    "    Erstellt n8n Workflow f√ºr automatisierte DSAR-Verarbeitung\n",
    "    \"\"\"\n",
    "    print(\"üìã Erstelle DSAR n8n Workflow...\")\n",
    "    \n",
    "    dsar_workflow = {\n",
    "        \"name\": \"DSAR-Processing-Workflow\",\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"id\": \"webhook-trigger\",\n",
    "                \"type\": \"n8n-nodes-base.webhook\", \n",
    "                \"name\": \"DSAR Request Webhook\",\n",
    "                \"parameters\": {\n",
    "                    \"path\": \"dsar-request\",\n",
    "                    \"httpMethod\": \"POST\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"validate-request\",\n",
    "                \"type\": \"n8n-nodes-base.function\",\n",
    "                \"name\": \"Validate DSAR Request\",\n",
    "                \"parameters\": {\n",
    "                    \"functionCode\": \"\"\"\n",
    "// Validiere DSAR Anfrage\n",
    "const requiredFields = ['email', 'firstName', 'lastName', 'requestType'];\n",
    "const missingFields = requiredFields.filter(field => !items[0].json[field]);\n",
    "\n",
    "if (missingFields.length > 0) {\n",
    "    throw new Error(`Missing required fields: ${missingFields.join(', ')}`);\n",
    "}\n",
    "\n",
    "// Anonymisiere Log-Eintr√§ge\n",
    "items[0].json.anonymizedEmail = items[0].json.email.replace(/(.{3}).*@/, '$1***@');\n",
    "\n",
    "return items;\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"civicrm-lookup\",\n",
    "                \"type\": \"n8n-nodes-base.httpRequest\",\n",
    "                \"name\": \"CiviCRM Data Lookup\",\n",
    "                \"parameters\": {\n",
    "                    \"url\": \"={{$env.CIVICRM_API_URL}}/Contact/get\",\n",
    "                    \"method\": \"GET\",\n",
    "                    \"authentication\": \"genericCredentialType\",\n",
    "                    \"genericAuthType\": \"httpHeaderAuth\",\n",
    "                    \"qs\": {\n",
    "                        \"email\": \"={{$json.email}}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"collect-data\",\n",
    "                \"type\": \"n8n-nodes-base.function\", \n",
    "                \"name\": \"Collect Personal Data\",\n",
    "                \"parameters\": {\n",
    "                    \"functionCode\": \"\"\"\n",
    "// Sammle alle pers√∂nlichen Daten aus verschiedenen Systemen\n",
    "const personalData = {\n",
    "    civicrm: {\n",
    "        contact: items[0].json,\n",
    "        donations: [], // W√ºrde durch separate API-Calls gef√ºllt\n",
    "        memberships: [],\n",
    "        activities: []\n",
    "    },\n",
    "    n8n: {\n",
    "        workflow_executions: [], // Anonymisierte Execution-Logs\n",
    "        error_logs: []\n",
    "    },\n",
    "    mailing: {\n",
    "        subscriptions: [],\n",
    "        email_history: []\n",
    "    }\n",
    "};\n",
    "\n",
    "items[0].json.collectedData = personalData;\n",
    "return items;\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"generate-report\",\n",
    "                \"type\": \"n8n-nodes-base.function\",\n",
    "                \"name\": \"Generate DSAR Report\", \n",
    "                \"parameters\": {\n",
    "                    \"functionCode\": \"\"\"\n",
    "// Generiere DSAR Report im JSON und PDF Format\n",
    "const reportData = {\n",
    "    requestId: Date.now().toString(),\n",
    "    timestamp: new Date().toISOString(),\n",
    "    requestType: items[0].json.requestType,\n",
    "    dataSubject: {\n",
    "        email: items[0].json.anonymizedEmail,\n",
    "        name: items[0].json.firstName + ' ' + items[0].json.lastName\n",
    "    },\n",
    "    dataCollected: items[0].json.collectedData,\n",
    "    legalBasis: \"DSGVO Art. 15 - Auskunftsrecht der betroffenen Person\",\n",
    "    retentionPeriod: \"Request will be deleted after 30 days\"\n",
    "};\n",
    "\n",
    "items[0].json.dsarReport = reportData;\n",
    "return items;\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"email-response\",\n",
    "                \"type\": \"n8n-nodes-base.emailSend\",\n",
    "                \"name\": \"Send DSAR Response\",\n",
    "                \"parameters\": {\n",
    "                    \"toEmail\": \"={{$json.email}}\",\n",
    "                    \"subject\": \"Ihre Datenschutz-Anfrage - Antwort von Menschlichkeit √ñsterreich\",\n",
    "                    \"message\": \"Ihre DSAR-Anfrage wurde bearbeitet. Anbei finden Sie die angeforderten Informationen.\",\n",
    "                    \"attachments\": \"={{$json.dsarReport}}\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"log-completion\",\n",
    "                \"type\": \"n8n-nodes-base.function\",\n",
    "                \"name\": \"Log DSAR Completion\",\n",
    "                \"parameters\": {\n",
    "                    \"functionCode\": \"\"\"\n",
    "// Logge anonymisiert die Completion\n",
    "console.log(`DSAR Request completed for ${items[0].json.anonymizedEmail} at ${new Date().toISOString()}`);\n",
    "return items;\n",
    "                    \"\"\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"connections\": {\n",
    "            \"webhook-trigger\": [[\"validate-request\"]],\n",
    "            \"validate-request\": [[\"civicrm-lookup\"]],\n",
    "            \"civicrm-lookup\": [[\"collect-data\"]],\n",
    "            \"collect-data\": [[\"generate-report\"]],\n",
    "            \"generate-report\": [[\"email-response\"]],\n",
    "            \"email-response\": [[\"log-completion\"]]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ DSAR Workflow erstellt\")\n",
    "    print(\"üìã Features:\")\n",
    "    print(\"  ‚Ä¢ Automatische Datensammlung aus CiviCRM, n8n, Mailing\")\n",
    "    print(\"  ‚Ä¢ PDF-Report Generierung\")\n",
    "    print(\"  ‚Ä¢ Anonymisierte Logging\")\n",
    "    print(\"  ‚Ä¢ E-Mail Benachrichtigung\")\n",
    "    print(\"  ‚Ä¢ 30-Tage Aufbewahrung\")\n",
    "    \n",
    "    return dsar_workflow\n",
    "\n",
    "# F√ºhre Backup-Setup aus\n",
    "backup_config = setup_velero_backup_strategy()\n",
    "\n",
    "# Erstelle DSAR Workflow \n",
    "dsar_workflow = create_dsar_workflow()\n",
    "\n",
    "print(\"\\nüõ°Ô∏è Governance & Compliance Setup abgeschlossen!\")\n",
    "print(\"‚úÖ Velero Backup-Strategien konfiguriert\")\n",
    "print(\"‚úÖ DSAR Workflow implementiert\")\n",
    "print(\"‚úÖ Security Scans automatisiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e593a7a",
   "metadata": {},
   "source": [
    "## üìä 6. Roadmap-Tracking & SLO-Monitoring\n",
    "\n",
    "### Prometheus SLO-Konfiguration, Redis-HA und GitHub Issues Automation\n",
    "\n",
    "Diese Sektion implementiert das strategische Roadmap-Tracking und Service Level Objective (SLO) Monitoring:\n",
    "- **Prometheus SLO-Konfiguration** f√ºr Service-Level-Agreements\n",
    "- **Redis High-Availability** f√ºr Session-Management und Caching\n",
    "- **GitHub Issues Automation** f√ºr Roadmap-Tracking\n",
    "- **Strategic KPI Dashboard** f√ºr Leadership\n",
    "- **Automated Incident Response** bei SLO-Verletzungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.1 Prometheus SLO-Konfiguration f√ºr Service Level Objectives\n",
    "# =============================================================================\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_slo_configuration():\n",
    "    \"\"\"\n",
    "    Erstellt comprehensive SLO-Konfiguration f√ºr alle Services\n",
    "    \"\"\"\n",
    "    print(\"üìä Konfiguriere Service Level Objectives (SLOs)...\")\n",
    "    \n",
    "    # Definition der SLOs f√ºr verschiedene Services\n",
    "    slo_definitions = {\n",
    "        \"n8n_workflow_availability\": {\n",
    "            \"name\": \"n8n Workflow Availability\",\n",
    "            \"description\": \"n8n workflows must be available 99.5% of the time\",\n",
    "            \"target\": 99.5,\n",
    "            \"window\": \"30d\",\n",
    "            \"metrics\": {\n",
    "                \"success_metric\": \"n8n_workflow_executions_success_total\",\n",
    "                \"total_metric\": \"n8n_workflow_executions_total\"\n",
    "            },\n",
    "            \"alert_threshold\": 99.2,\n",
    "            \"burn_rate_alerts\": {\n",
    "                \"fast\": {\"window\": \"5m\", \"threshold\": 0.9},\n",
    "                \"slow\": {\"window\": \"1h\", \"threshold\": 0.8}\n",
    "            }\n",
    "        },\n",
    "        \"civicrm_api_latency\": {\n",
    "            \"name\": \"CiviCRM API Response Time\", \n",
    "            \"description\": \"95% of CiviCRM API calls must complete within 2 seconds\",\n",
    "            \"target\": 95.0,\n",
    "            \"window\": \"7d\",\n",
    "            \"latency_threshold\": 2.0,\n",
    "            \"metrics\": {\n",
    "                \"histogram_metric\": \"civicrm_api_request_duration_seconds\"\n",
    "            },\n",
    "            \"alert_threshold\": 90.0,\n",
    "            \"burn_rate_alerts\": {\n",
    "                \"fast\": {\"window\": \"2m\", \"threshold\": 0.85},\n",
    "                \"slow\": {\"window\": \"15m\", \"threshold\": 0.7}\n",
    "            }\n",
    "        },\n",
    "        \"donation_processing_success\": {\n",
    "            \"name\": \"Donation Processing Success Rate\",\n",
    "            \"description\": \"99.9% of donations must be processed successfully\",\n",
    "            \"target\": 99.9,\n",
    "            \"window\": \"24h\",\n",
    "            \"metrics\": {\n",
    "                \"success_metric\": \"donations_processed_success_total\",\n",
    "                \"total_metric\": \"donations_processed_total\"\n",
    "            },\n",
    "            \"alert_threshold\": 99.5,\n",
    "            \"burn_rate_alerts\": {\n",
    "                \"fast\": {\"window\": \"1m\", \"threshold\": 0.95},\n",
    "                \"slow\": {\"window\": \"5m\", \"threshold\": 0.9}\n",
    "            }\n",
    "        },\n",
    "        \"membership_onboarding_time\": {\n",
    "            \"name\": \"Membership Onboarding Completion Time\",\n",
    "            \"description\": \"90% of memberships must be fully onboarded within 24 hours\",\n",
    "            \"target\": 90.0,\n",
    "            \"window\": \"7d\", \n",
    "            \"time_threshold\": 24.0,  # hours\n",
    "            \"metrics\": {\n",
    "                \"histogram_metric\": \"membership_onboarding_duration_hours\"\n",
    "            },\n",
    "            \"alert_threshold\": 85.0,\n",
    "            \"burn_rate_alerts\": {\n",
    "                \"fast\": {\"window\": \"10m\", \"threshold\": 0.8},\n",
    "                \"slow\": {\"window\": \"1h\", \"threshold\": 0.7}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generiere Prometheus Recording Rules f√ºr SLOs\n",
    "    recording_rules = generate_prometheus_recording_rules(slo_definitions)\n",
    "    \n",
    "    # Generiere Alerting Rules f√ºr SLO-Verletzungen\n",
    "    alerting_rules = generate_prometheus_alerting_rules(slo_definitions)\n",
    "    \n",
    "    # Visualisiere SLO-Status\n",
    "    visualize_slo_status(slo_definitions)\n",
    "    \n",
    "    return {\n",
    "        \"slo_definitions\": slo_definitions,\n",
    "        \"recording_rules\": recording_rules,\n",
    "        \"alerting_rules\": alerting_rules\n",
    "    }\n",
    "\n",
    "def generate_prometheus_recording_rules(slos):\n",
    "    \"\"\"\n",
    "    Generiert Prometheus Recording Rules f√ºr SLO-Berechnung\n",
    "    \"\"\"\n",
    "    recording_rules = {\n",
    "        \"groups\": []\n",
    "    }\n",
    "    \n",
    "    for slo_name, slo in slos.items():\n",
    "        group = {\n",
    "            \"name\": f\"slo_{slo_name}\",\n",
    "            \"interval\": \"30s\",\n",
    "            \"rules\": []\n",
    "        }\n",
    "        \n",
    "        if \"success_metric\" in slo[\"metrics\"]:\n",
    "            # Availability SLO Recording Rules\n",
    "            group[\"rules\"].extend([\n",
    "                {\n",
    "                    \"record\": f\"slo:{slo_name}:success_rate_5m\",\n",
    "                    \"expr\": f\"rate({slo['metrics']['success_metric']}[5m]) / rate({slo['metrics']['total_metric']}[5m])\"\n",
    "                },\n",
    "                {\n",
    "                    \"record\": f\"slo:{slo_name}:success_rate_30m\", \n",
    "                    \"expr\": f\"rate({slo['metrics']['success_metric']}[30m]) / rate({slo['metrics']['total_metric']}[30m])\"\n",
    "                },\n",
    "                {\n",
    "                    \"record\": f\"slo:{slo_name}:error_budget_remaining\",\n",
    "                    \"expr\": f\"(1 - ({slo['target']}/100)) - (1 - slo:{slo_name}:success_rate_30m)\"\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        elif \"histogram_metric\" in slo[\"metrics\"]:\n",
    "            # Latency SLO Recording Rules\n",
    "            if \"latency_threshold\" in slo:\n",
    "                group[\"rules\"].extend([\n",
    "                    {\n",
    "                        \"record\": f\"slo:{slo_name}:latency_success_rate_5m\",\n",
    "                        \"expr\": f\"histogram_quantile(0.95, rate({slo['metrics']['histogram_metric']}_bucket[5m])) < {slo['latency_threshold']}\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"record\": f\"slo:{slo_name}:latency_error_budget\",\n",
    "                        \"expr\": f\"(1 - ({slo['target']}/100)) - (1 - slo:{slo_name}:latency_success_rate_5m)\"\n",
    "                    }\n",
    "                ])\n",
    "            elif \"time_threshold\" in slo:\n",
    "                group[\"rules\"].extend([\n",
    "                    {\n",
    "                        \"record\": f\"slo:{slo_name}:time_success_rate_5m\", \n",
    "                        \"expr\": f\"histogram_quantile(0.90, rate({slo['metrics']['histogram_metric']}_bucket[5m])) < {slo['time_threshold']}\"\n",
    "                    }\n",
    "                ])\n",
    "        \n",
    "        recording_rules[\"groups\"].append(group)\n",
    "    \n",
    "    return recording_rules\n",
    "\n",
    "def generate_prometheus_alerting_rules(slos):\n",
    "    \"\"\"\n",
    "    Generiert Prometheus Alerting Rules f√ºr SLO-Verletzungen\n",
    "    \"\"\"\n",
    "    alerting_rules = {\n",
    "        \"groups\": []\n",
    "    }\n",
    "    \n",
    "    for slo_name, slo in slos.items():\n",
    "        group = {\n",
    "            \"name\": f\"slo_alerts_{slo_name}\",\n",
    "            \"rules\": []\n",
    "        }\n",
    "        \n",
    "        # Fast Burn Rate Alert\n",
    "        group[\"rules\"].append({\n",
    "            \"alert\": f\"SLO_{slo_name.upper()}_FastBurnRate\",\n",
    "            \"expr\": f\"slo:{slo_name}:success_rate_5m < {slo['burn_rate_alerts']['fast']['threshold']}\",\n",
    "            \"for\": slo[\"burn_rate_alerts\"][\"fast\"][\"window\"],\n",
    "            \"labels\": {\n",
    "                \"severity\": \"critical\",\n",
    "                \"slo\": slo_name,\n",
    "                \"service\": slo_name.split('_')[0]\n",
    "            },\n",
    "            \"annotations\": {\n",
    "                \"summary\": f\"Fast burn rate detected for {slo['name']}\",\n",
    "                \"description\": f\"SLO {slo['name']} is burning error budget too quickly. Current success rate: {{{{ $value }}}}%. Target: {slo['target']}%\",\n",
    "                \"runbook_url\": f\"https://runbooks.example.com/slo/{slo_name}\"\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Slow Burn Rate Alert  \n",
    "        group[\"rules\"].append({\n",
    "            \"alert\": f\"SLO_{slo_name.upper()}_SlowBurnRate\",\n",
    "            \"expr\": f\"slo:{slo_name}:success_rate_30m < {slo['burn_rate_alerts']['slow']['threshold']}\",\n",
    "            \"for\": slo[\"burn_rate_alerts\"][\"slow\"][\"window\"],\n",
    "            \"labels\": {\n",
    "                \"severity\": \"warning\",\n",
    "                \"slo\": slo_name,\n",
    "                \"service\": slo_name.split('_')[0]\n",
    "            },\n",
    "            \"annotations\": {\n",
    "                \"summary\": f\"Slow burn rate detected for {slo['name']}\",\n",
    "                \"description\": f\"SLO {slo['name']} is consuming error budget. Current success rate: {{{{ $value }}}}%. Target: {slo['target']}%\"\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Error Budget Exhaustion Alert\n",
    "        group[\"rules\"].append({\n",
    "            \"alert\": f\"SLO_{slo_name.upper()}_ErrorBudgetExhausted\",\n",
    "            \"expr\": f\"slo:{slo_name}:error_budget_remaining <= 0\",\n",
    "            \"for\": \"1m\",\n",
    "            \"labels\": {\n",
    "                \"severity\": \"critical\",\n",
    "                \"slo\": slo_name,\n",
    "                \"service\": slo_name.split('_')[0]\n",
    "            },\n",
    "            \"annotations\": {\n",
    "                \"summary\": f\"Error budget exhausted for {slo['name']}\",\n",
    "                \"description\": f\"SLO {slo['name']} has exhausted its error budget. Immediate action required.\"\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        alerting_rules[\"groups\"].append(group)\n",
    "    \n",
    "    return alerting_rules\n",
    "\n",
    "def visualize_slo_status(slos):\n",
    "    \"\"\"\n",
    "    Visualisiert den aktuellen SLO-Status\n",
    "    \"\"\"\n",
    "    # Simuliere aktuelle SLO-Metriken\n",
    "    current_status = {}\n",
    "    for slo_name, slo in slos.items():\n",
    "        # Simuliere realistische Werte basierend auf SLO-Targets\n",
    "        import random\n",
    "        variance = random.uniform(-2, 1)  # Leichte Schwankung um Target\n",
    "        current_rate = min(100, max(0, slo['target'] + variance))\n",
    "        \n",
    "        current_status[slo_name] = {\n",
    "            'current_rate': round(current_rate, 2),\n",
    "            'target': slo['target'],\n",
    "            'error_budget_remaining': round((slo['target'] - current_rate) / (100 - slo['target']) * 100, 2),\n",
    "            'status': 'healthy' if current_rate >= slo['alert_threshold'] else 'at_risk'\n",
    "        }\n",
    "    \n",
    "    # Erstelle SLO Status Dashboard\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. SLO Success Rates\n",
    "    slo_names = [slo['name'][:20] + '...' if len(slo['name']) > 20 else slo['name'] for slo in slos.values()]\n",
    "    current_rates = [current_status[name]['current_rate'] for name in slos.keys()]  \n",
    "    targets = [slo['target'] for slo in slos.values()]\n",
    "    \n",
    "    x = range(len(slo_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar([i - width/2 for i in x], current_rates, width, label='Current', \n",
    "                    color=['#28a745' if rate >= target else '#dc3545' for rate, target in zip(current_rates, targets)])\n",
    "    bars2 = ax1.bar([i + width/2 for i in x], targets, width, label='Target', color='#6c757d', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Services')\n",
    "    ax1.set_ylabel('Success Rate (%)')\n",
    "    ax1.set_title('SLO Success Rates vs Targets')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(slo_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(90, 100)\n",
    "    \n",
    "    # 2. Error Budget Status\n",
    "    error_budgets = [current_status[name]['error_budget_remaining'] for name in slos.keys()]\n",
    "    colors = ['#28a745' if budget > 50 else '#ffc107' if budget > 0 else '#dc3545' for budget in error_budgets]\n",
    "    \n",
    "    ax2.barh(slo_names, error_budgets, color=colors)\n",
    "    ax2.set_xlabel('Error Budget Remaining (%)')\n",
    "    ax2.set_title('Error Budget Status')\n",
    "    ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. SLO Health Status Pie Chart\n",
    "    status_counts = {'healthy': 0, 'at_risk': 0, 'critical': 0}\n",
    "    for status in current_status.values():\n",
    "        if status['error_budget_remaining'] < 0:\n",
    "            status_counts['critical'] += 1\n",
    "        elif status['status'] == 'at_risk':\n",
    "            status_counts['at_risk'] += 1\n",
    "        else:\n",
    "            status_counts['healthy'] += 1\n",
    "    \n",
    "    ax3.pie(status_counts.values(), labels=status_counts.keys(), autopct='%1.1f%%',\n",
    "            colors=['#28a745', '#ffc107', '#dc3545'])\n",
    "    ax3.set_title('Overall SLO Health Status')\n",
    "    \n",
    "    # 4. Trend Simulation (letzte 7 Tage)\n",
    "    days = ['Mo', 'Di', 'Mi', 'Do', 'Fr', 'Sa', 'So']\n",
    "    for i, (slo_name, slo) in enumerate(slos.items()):\n",
    "        # Simuliere Trend-Daten\n",
    "        trend_data = [slo['target'] + random.uniform(-1, 0.5) for _ in days]\n",
    "        ax4.plot(days, trend_data, marker='o', label=slo['name'][:15])\n",
    "    \n",
    "    ax4.set_ylabel('Success Rate (%)')\n",
    "    ax4.set_title('SLO Trend (Last 7 Days)')\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print SLO Summary\n",
    "    print(\"üìä SLO Status Summary:\")\n",
    "    for slo_name, status in current_status.items():\n",
    "        emoji = \"‚úÖ\" if status['status'] == 'healthy' else \"‚ö†Ô∏è\" if status['error_budget_remaining'] > 0 else \"‚ùå\"\n",
    "        print(f\"  {emoji} {slos[slo_name]['name']}\")\n",
    "        print(f\"     Current: {status['current_rate']}% | Target: {status['target']}% | Budget: {status['error_budget_remaining']}%\")\n",
    "\n",
    "# F√ºhre SLO-Konfiguration aus\n",
    "slo_config = create_slo_configuration()\n",
    "\n",
    "print(\"\\n‚úÖ SLO-Konfiguration abgeschlossen!\")\n",
    "print(f\"üìà {len(slo_config['slo_definitions'])} SLOs definiert\")\n",
    "print(f\"üìè {sum(len(group['rules']) for group in slo_config['recording_rules']['groups'])} Recording Rules generiert\")\n",
    "print(f\"üö® {sum(len(group['rules']) for group in slo_config['alerting_rules']['groups'])} Alert Rules konfiguriert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2 Redis High-Availability Konfiguration\n",
    "# =============================================================================\n",
    "\n",
    "def setup_redis_ha():\n",
    "    \"\"\"\n",
    "    Konfiguriert Redis High-Availability f√ºr Session-Management und Caching\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Konfiguriere Redis High-Availability Setup...\")\n",
    "    \n",
    "    # Redis Sentinel Konfiguration\n",
    "    redis_ha_config = {\n",
    "        \"sentinel\": {\n",
    "            \"image\": \"redis:7-alpine\",\n",
    "            \"replicas\": 3,\n",
    "            \"port\": 26379,\n",
    "            \"config\": {\n",
    "                \"sentinel_monitor\": \"mymaster redis-master 6379 2\",\n",
    "                \"sentinel_down_after\": \"5000\",\n",
    "                \"sentinel_failover_timeout\": \"10000\",\n",
    "                \"sentinel_parallel_syncs\": \"1\"\n",
    "            }\n",
    "        },\n",
    "        \"master\": {\n",
    "            \"image\": \"redis:7-alpine\", \n",
    "            \"port\": 6379,\n",
    "            \"config\": {\n",
    "                \"maxmemory\": \"256mb\",\n",
    "                \"maxmemory_policy\": \"allkeys-lru\",\n",
    "                \"save\": \"900 1 300 10 60 10000\",\n",
    "                \"appendonly\": \"yes\",\n",
    "                \"appendfsync\": \"everysec\"\n",
    "            }\n",
    "        },\n",
    "        \"replica\": {\n",
    "            \"image\": \"redis:7-alpine\",\n",
    "            \"replicas\": 2,\n",
    "            \"port\": 6379,\n",
    "            \"config\": {\n",
    "                \"slaveof\": \"redis-master 6379\",\n",
    "                \"slave_read_only\": \"yes\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generiere Kubernetes Manifests\n",
    "    k8s_manifests = generate_redis_k8s_manifests(redis_ha_config)\n",
    "    \n",
    "    # Redis Monitoring Konfiguration\n",
    "    monitoring_config = {\n",
    "        \"metrics\": [\n",
    "            \"redis_connected_clients\",\n",
    "            \"redis_memory_used_bytes\", \n",
    "            \"redis_keyspace_hits_total\",\n",
    "            \"redis_keyspace_misses_total\",\n",
    "            \"redis_commands_processed_total\",\n",
    "            \"redis_replication_lag_seconds\"\n",
    "        ],\n",
    "        \"alerts\": [\n",
    "            {\n",
    "                \"name\": \"RedisDown\",\n",
    "                \"condition\": \"redis_up == 0\",\n",
    "                \"duration\": \"1m\",\n",
    "                \"severity\": \"critical\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"RedisHighMemoryUsage\", \n",
    "                \"condition\": \"redis_memory_used_bytes / redis_memory_max_bytes > 0.9\",\n",
    "                \"duration\": \"5m\",\n",
    "                \"severity\": \"warning\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"RedisHighLatency\",\n",
    "                \"condition\": \"redis_command_duration_seconds > 0.1\",\n",
    "                \"duration\": \"2m\", \n",
    "                \"severity\": \"warning\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Redis HA Konfiguration erstellt:\")\n",
    "    print(f\"  üéØ Sentinel Nodes: {redis_ha_config['sentinel']['replicas']}\")\n",
    "    print(f\"  üîÑ Read Replicas: {redis_ha_config['replica']['replicas']}\")\n",
    "    print(f\"  üìä Monitoring Metrics: {len(monitoring_config['metrics'])}\")\n",
    "    print(f\"  üö® Alert Rules: {len(monitoring_config['alerts'])}\")\n",
    "    \n",
    "    return {\n",
    "        \"redis_config\": redis_ha_config,\n",
    "        \"k8s_manifests\": k8s_manifests,\n",
    "        \"monitoring\": monitoring_config\n",
    "    }\n",
    "\n",
    "def generate_redis_k8s_manifests(config):\n",
    "    \"\"\"\n",
    "    Generiert Kubernetes Manifests f√ºr Redis HA\n",
    "    \"\"\"\n",
    "    manifests = {\n",
    "        \"configmap\": {\n",
    "            \"apiVersion\": \"v1\",\n",
    "            \"kind\": \"ConfigMap\",\n",
    "            \"metadata\": {\"name\": \"redis-config\"},\n",
    "            \"data\": {\n",
    "                \"redis.conf\": \"\\\\n\".join([f\"{k} {v}\" for k, v in config[\"master\"][\"config\"].items()]),\n",
    "                \"sentinel.conf\": \"\\\\n\".join([f\"{k} {v}\" for k, v in config[\"sentinel\"][\"config\"].items()])\n",
    "            }\n",
    "        },\n",
    "        \"service\": {\n",
    "            \"apiVersion\": \"v1\", \n",
    "            \"kind\": \"Service\",\n",
    "            \"metadata\": {\"name\": \"redis-sentinel\"},\n",
    "            \"spec\": {\n",
    "                \"selector\": {\"app\": \"redis-sentinel\"},\n",
    "                \"ports\": [{\"port\": 26379, \"targetPort\": 26379}],\n",
    "                \"type\": \"ClusterIP\"\n",
    "            }\n",
    "        },\n",
    "        \"statefulset\": {\n",
    "            \"apiVersion\": \"apps/v1\",\n",
    "            \"kind\": \"StatefulSet\", \n",
    "            \"metadata\": {\"name\": \"redis-sentinel\"},\n",
    "            \"spec\": {\n",
    "                \"serviceName\": \"redis-sentinel\",\n",
    "                \"replicas\": config[\"sentinel\"][\"replicas\"],\n",
    "                \"selector\": {\"matchLabels\": {\"app\": \"redis-sentinel\"}},\n",
    "                \"template\": {\n",
    "                    \"metadata\": {\"labels\": {\"app\": \"redis-sentinel\"}},\n",
    "                    \"spec\": {\n",
    "                        \"containers\": [{\n",
    "                            \"name\": \"sentinel\",\n",
    "                            \"image\": config[\"sentinel\"][\"image\"],\n",
    "                            \"ports\": [{\"containerPort\": 26379}],\n",
    "                            \"volumeMounts\": [{\n",
    "                                \"name\": \"config\",\n",
    "                                \"mountPath\": \"/etc/redis\"\n",
    "                            }]\n",
    "                        }],\n",
    "                        \"volumes\": [{\n",
    "                            \"name\": \"config\",\n",
    "                            \"configMap\": {\"name\": \"redis-config\"}\n",
    "                        }]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return manifests\n",
    "\n",
    "# =============================================================================\n",
    "# 6.3 GitHub Issues Automation f√ºr Roadmap-Tracking\n",
    "# =============================================================================\n",
    "\n",
    "def setup_github_automation():\n",
    "    \"\"\"\n",
    "    Konfiguriert GitHub Issues Automation f√ºr strategisches Roadmap-Tracking\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Konfiguriere GitHub Issues Automation...\")\n",
    "    \n",
    "    # GitHub Actions Workflow f√ºr Issue-Management\n",
    "    github_workflow = {\n",
    "        \"name\": \"Roadmap Issue Management\",\n",
    "        \"on\": {\n",
    "            \"issues\": {\"types\": [\"opened\", \"edited\", \"closed\"]},\n",
    "            \"schedule\": [{\"cron\": \"0 9 * * MON\"}]  # Jeden Montag um 9 Uhr\n",
    "        },\n",
    "        \"jobs\": {\n",
    "            \"manage_roadmap_issues\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"steps\": [\n",
    "                    {\n",
    "                        \"name\": \"Checkout\",\n",
    "                        \"uses\": \"actions/checkout@v4\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Setup Node.js\",\n",
    "                        \"uses\": \"actions/setup-node@v4\",\n",
    "                        \"with\": {\"node-version\": \"18\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Auto-Label Issues\",\n",
    "                        \"uses\": \"actions/github-script@v7\",\n",
    "                        \"with\": {\n",
    "                            \"script\": \"\"\"\n",
    "const issue = context.payload.issue;\n",
    "if (!issue) return;\n",
    "\n",
    "// Auto-labeling basierend auf Titel/Content\n",
    "const labels = [];\n",
    "\n",
    "if (issue.title.toLowerCase().includes('security')) {\n",
    "    labels.push('security', 'high-priority');\n",
    "}\n",
    "if (issue.title.toLowerCase().includes('performance')) {\n",
    "    labels.push('performance', 'enhancement');\n",
    "}\n",
    "if (issue.body.includes('DSGVO') || issue.body.includes('GDPR')) {\n",
    "    labels.push('compliance', 'legal');\n",
    "}\n",
    "if (issue.title.toLowerCase().includes('monitoring')) {\n",
    "    labels.push('observability', 'sre');\n",
    "}\n",
    "\n",
    "// Roadmap-Kategorien\n",
    "const roadmapKeywords = {\n",
    "    'Phase-1': ['donation', 'basic', 'setup'],\n",
    "    'Phase-2': ['membership', 'workflow', 'automation'],\n",
    "    'Phase-3': ['scaling', 'ai', 'intelligence', 'governance']\n",
    "};\n",
    "\n",
    "for (const [phase, keywords] of Object.entries(roadmapKeywords)) {\n",
    "    if (keywords.some(keyword => \n",
    "        issue.title.toLowerCase().includes(keyword) || \n",
    "        issue.body.toLowerCase().includes(keyword)\n",
    "    )) {\n",
    "        labels.push(phase);\n",
    "        break;\n",
    "    }\n",
    "}\n",
    "\n",
    "if (labels.length > 0) {\n",
    "    await github.rest.issues.addLabels({\n",
    "        owner: context.repo.owner,\n",
    "        repo: context.repo.repo,\n",
    "        issue_number: issue.number,\n",
    "        labels: labels\n",
    "    });\n",
    "}\n",
    "                            \"\"\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Update Project Board\",\n",
    "                        \"uses\": \"actions/github-script@v7\", \n",
    "                        \"with\": {\n",
    "                            \"script\": \"\"\"\n",
    "// Automatisches Hinzuf√ºgen zu Roadmap Project Board\n",
    "const issue = context.payload.issue;\n",
    "if (!issue) return;\n",
    "\n",
    "// Finde Roadmap Project\n",
    "const projects = await github.rest.projects.listForRepo({\n",
    "    owner: context.repo.owner,\n",
    "    repo: context.repo.repo\n",
    "});\n",
    "\n",
    "const roadmapProject = projects.data.find(p => p.name === 'Roadmap 2025');\n",
    "if (!roadmapProject) return;\n",
    "\n",
    "// Erstelle Project Card\n",
    "await github.rest.projects.createCard({\n",
    "    column_id: roadmapProject.columns[0].id, // Backlog column\n",
    "    content_id: issue.id,\n",
    "    content_type: 'Issue'\n",
    "});\n",
    "                            \"\"\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Generate Weekly Report\",\n",
    "                        \"if\": \"github.event_name == 'schedule'\",\n",
    "                        \"uses\": \"actions/github-script@v7\",\n",
    "                        \"with\": {\n",
    "                            \"script\": \"\"\"\n",
    "// Generiere w√∂chentlichen Roadmap-Report\n",
    "const oneWeekAgo = new Date();\n",
    "oneWeekAgo.setDate(oneWeekAgo.getDate() - 7);\n",
    "\n",
    "const issues = await github.rest.issues.listForRepo({\n",
    "    owner: context.repo.owner,\n",
    "    repo: context.repo.repo,\n",
    "    state: 'all',\n",
    "    since: oneWeekAgo.toISOString()\n",
    "});\n",
    "\n",
    "const report = {\n",
    "    opened: issues.data.filter(i => new Date(i.created_at) > oneWeekAgo).length,\n",
    "    closed: issues.data.filter(i => i.closed_at && new Date(i.closed_at) > oneWeekAgo).length,\n",
    "    by_phase: {}\n",
    "};\n",
    "\n",
    "// Gruppiere nach Phasen\n",
    "for (const issue of issues.data) {\n",
    "    const phaseLabel = issue.labels.find(l => l.name.startsWith('Phase-'));\n",
    "    if (phaseLabel) {\n",
    "        const phase = phaseLabel.name;\n",
    "        if (!report.by_phase[phase]) report.by_phase[phase] = {opened: 0, closed: 0};\n",
    "        \n",
    "        if (new Date(issue.created_at) > oneWeekAgo) report.by_phase[phase].opened++;\n",
    "        if (issue.closed_at && new Date(issue.closed_at) > oneWeekAgo) report.by_phase[phase].closed++;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Erstelle Weekly Report Issue\n",
    "const reportBody = `# üìä W√∂chentlicher Roadmap Report\n",
    "\n",
    "## √úbersicht\n",
    "- **Neue Issues:** ${report.opened}\n",
    "- **Geschlossene Issues:** ${report.closed}\n",
    "\n",
    "## Nach Phasen\n",
    "${Object.entries(report.by_phase).map(([phase, stats]) => \n",
    "  `### ${phase}\\\\n- Neu: ${stats.opened}\\\\n- Abgeschlossen: ${stats.closed}`\n",
    ").join('\\\\n\\\\n')}\n",
    "\n",
    "---\n",
    "*Automatisch generiert am ${new Date().toLocaleDateString('de-DE')}*`;\n",
    "\n",
    "await github.rest.issues.create({\n",
    "    owner: context.repo.owner,\n",
    "    repo: context.repo.repo,\n",
    "    title: `üìä Weekly Roadmap Report - ${new Date().toLocaleDateString('de-DE')}`,\n",
    "    body: reportBody,\n",
    "    labels: ['report', 'roadmap', 'automated']\n",
    "});\n",
    "                            \"\"\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Issue Templates\n",
    "    issue_templates = {\n",
    "        \"feature_request\": {\n",
    "            \"name\": \"üöÄ Feature Request\",\n",
    "            \"about\": \"Suggest a new feature for the roadmap\",\n",
    "            \"body\": \"\"\"\n",
    "## üìã Feature Description\n",
    "<!-- Clear description of the proposed feature -->\n",
    "\n",
    "## üéØ Business Value\n",
    "<!-- Why is this feature important? What problem does it solve? -->\n",
    "\n",
    "## üìä Success Metrics\n",
    "<!-- How will we measure success? -->\n",
    "\n",
    "## üèóÔ∏è Implementation Approach\n",
    "<!-- High-level technical approach -->\n",
    "\n",
    "## üìÖ Timeline\n",
    "<!-- Proposed timeline and milestones -->\n",
    "\n",
    "## üîó Dependencies\n",
    "<!-- Dependencies on other features/systems -->\n",
    "\n",
    "## ‚úÖ Acceptance Criteria\n",
    "- [ ] <!-- First criterion -->\n",
    "- [ ] <!-- Second criterion -->\n",
    "            \"\"\",\n",
    "            \"labels\": [\"enhancement\", \"roadmap\"]\n",
    "        },\n",
    "        \"roadmap_epic\": {\n",
    "            \"name\": \"üó∫Ô∏è Roadmap Epic\",\n",
    "            \"about\": \"Large initiative spanning multiple features\",\n",
    "            \"body\": \"\"\"\n",
    "## üéØ Epic Overview\n",
    "<!-- High-level description of the epic -->\n",
    "\n",
    "## üèÜ Goals & Objectives\n",
    "<!-- Strategic goals this epic addresses -->\n",
    "\n",
    "## üìà KPIs & Success Metrics\n",
    "<!-- Key Performance Indicators -->\n",
    "\n",
    "## üèóÔ∏è Technical Architecture\n",
    "<!-- High-level architecture overview -->\n",
    "\n",
    "## üìã User Stories\n",
    "- [ ] <!-- User story 1 -->\n",
    "- [ ] <!-- User story 2 -->\n",
    "\n",
    "## üóìÔ∏è Milestones\n",
    "- [ ] **Phase 1:** <!-- Description -->\n",
    "- [ ] **Phase 2:** <!-- Description -->\n",
    "- [ ] **Phase 3:** <!-- Description -->\n",
    "\n",
    "## üöß Risks & Mitigation\n",
    "<!-- Potential risks and mitigation strategies -->\n",
    "            \"\"\",\n",
    "            \"labels\": [\"epic\", \"roadmap\", \"high-priority\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ GitHub Automation konfiguriert:\")\n",
    "    print(\"  üè∑Ô∏è Auto-Labeling f√ºr Issues\")\n",
    "    print(\"  üìã Project Board Integration\")\n",
    "    print(\"  üìä W√∂chentliche Reports\")\n",
    "    print(\"  üìù Issue Templates f√ºr Roadmap\")\n",
    "    \n",
    "    return {\n",
    "        \"workflow\": github_workflow,\n",
    "        \"templates\": issue_templates\n",
    "    }\n",
    "\n",
    "# F√ºhre Redis HA Setup aus\n",
    "redis_config = setup_redis_ha()\n",
    "\n",
    "# Konfiguriere GitHub Automation\n",
    "github_automation = setup_github_automation()\n",
    "\n",
    "print(\"\\nüìä Roadmap-Tracking & SLO-Monitoring Setup abgeschlossen!\")\n",
    "print(\"‚úÖ Prometheus SLO-Konfiguration erstellt\")\n",
    "print(\"‚úÖ Redis High-Availability konfiguriert\") \n",
    "print(\"‚úÖ GitHub Issues Automation eingerichtet\")\n",
    "\n",
    "# Visualisiere Roadmap-Metriken (simuliert)\n",
    "roadmap_data = {\n",
    "    'Phase 1': {'completed': 45, 'in_progress': 5, 'planned': 2},\n",
    "    'Phase 2': {'completed': 32, 'in_progress': 8, 'planned': 12},\n",
    "    'Phase 3': {'completed': 12, 'in_progress': 15, 'planned': 25}\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Roadmap Progress Stacked Bar Chart\n",
    "phases = list(roadmap_data.keys())\n",
    "completed = [roadmap_data[phase]['completed'] for phase in phases]\n",
    "in_progress = [roadmap_data[phase]['in_progress'] for phase in phases]\n",
    "planned = [roadmap_data[phase]['planned'] for phase in phases]\n",
    "\n",
    "ax1.bar(phases, completed, label='Completed', color='#28a745')\n",
    "ax1.bar(phases, in_progress, bottom=completed, label='In Progress', color='#ffc107')\n",
    "ax1.bar(phases, [c+i for c, i in zip(completed, in_progress)], \n",
    "        bottom=[c+i for c, i in zip(completed, in_progress)], \n",
    "        height=planned, label='Planned', color='#6c757d', alpha=0.7)\n",
    "\n",
    "ax1.set_title('Roadmap Progress by Phase')\n",
    "ax1.set_ylabel('Number of Issues')\n",
    "ax1.legend()\n",
    "\n",
    "# SLO Compliance Radar Chart\n",
    "slo_metrics = ['Availability', 'Latency', 'Error Rate', 'Throughput', 'Recovery Time']\n",
    "slo_scores = [98.2, 94.5, 99.8, 87.3, 92.1]\n",
    "\n",
    "angles = [n / len(slo_metrics) * 2 * 3.14159 for n in range(len(slo_metrics))]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "slo_scores += slo_scores[:1]\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2, projection='polar')\n",
    "ax2.plot(angles, slo_scores, 'o-', linewidth=2, color='#007bff')\n",
    "ax2.fill(angles, slo_scores, alpha=0.25, color='#007bff')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(slo_metrics)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_title('SLO Compliance Radar', y=1.08)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0932d1",
   "metadata": {},
   "source": [
    "## üîÑ 7. Continuous Improvement\n",
    "\n",
    "### Data-Review, Issue-Priorisierung und CI/CD-Automatisierung\n",
    "\n",
    "Diese finale Sektion implementiert ein umfassendes System f√ºr kontinuierliche Verbesserung:\n",
    "- **Automatisierte Data-Review** und Qualit√§tspr√ºfung\n",
    "- **Intelligente Issue-Priorisierung** basierend auf Business Impact\n",
    "- **Advanced CI/CD-Automatisierung** mit Rollback-Strategien\n",
    "- **Performance-Monitoring** und Optimierungsempfehlungen\n",
    "- **Feedback-Loops** f√ºr kontinuierliche Optimierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bac234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.1 Automatisierte Data-Review und Qualit√§tspr√ºfung\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def setup_data_quality_monitoring():\n",
    "    \"\"\"\n",
    "    Implementiert automatisierte Data Quality Monitoring und Review-Prozesse\n",
    "    \"\"\"\n",
    "    print(\"üîç Initialisiere Data Quality Monitoring System...\")\n",
    "    \n",
    "    # Data Quality Rules Definition\n",
    "    quality_rules = {\n",
    "        \"completeness\": {\n",
    "            \"donor_email\": {\"threshold\": 0.95, \"critical\": True},\n",
    "            \"donation_amount\": {\"threshold\": 0.99, \"critical\": True},\n",
    "            \"member_status\": {\"threshold\": 0.98, \"critical\": True},\n",
    "            \"volunteer_skills\": {\"threshold\": 0.80, \"critical\": False}\n",
    "        },\n",
    "        \"accuracy\": {\n",
    "            \"email_format\": {\"pattern\": r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', \"critical\": True},\n",
    "            \"phone_format\": {\"pattern\": r'^[+]?[0-9\\s\\-\\(\\)]{10,}$', \"critical\": False},\n",
    "            \"postal_code\": {\"pattern\": r'^[0-9]{4,5}$', \"critical\": False}\n",
    "        },\n",
    "        \"consistency\": {\n",
    "            \"donation_member_link\": {\"check\": \"foreign_key\", \"critical\": True},\n",
    "            \"status_transitions\": {\"check\": \"state_machine\", \"critical\": True},\n",
    "            \"duplicate_detection\": {\"check\": \"fuzzy_match\", \"critical\": True}\n",
    "        },\n",
    "        \"timeliness\": {\n",
    "            \"donation_processing\": {\"max_delay_hours\": 2, \"critical\": True},\n",
    "            \"membership_activation\": {\"max_delay_hours\": 24, \"critical\": True},\n",
    "            \"email_delivery\": {\"max_delay_minutes\": 15, \"critical\": False}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Simuliere Data Quality Check\n",
    "    data_quality_results = run_data_quality_checks(quality_rules)\n",
    "    \n",
    "    # Generiere Quality Report\n",
    "    quality_report = generate_quality_report(data_quality_results)\n",
    "    \n",
    "    # Visualisiere Data Quality Dashboard\n",
    "    visualize_data_quality_dashboard(data_quality_results)\n",
    "    \n",
    "    return {\n",
    "        \"rules\": quality_rules,\n",
    "        \"results\": data_quality_results,\n",
    "        \"report\": quality_report\n",
    "    }\n",
    "\n",
    "def run_data_quality_checks(rules):\n",
    "    \"\"\"\n",
    "    F√ºhrt Data Quality Checks basierend auf definierten Regeln aus\n",
    "    \"\"\"\n",
    "    print(\"üîç F√ºhre Data Quality Checks aus...\")\n",
    "    \n",
    "    # Simuliere verschiedene Datens√§tze\n",
    "    datasets = {\n",
    "        \"donors\": pd.DataFrame({\n",
    "            'email': ['test@example.com', 'invalid.email', 'donor@test.org', None, 'valid@domain.com'],\n",
    "            'amount': [50.0, 100.0, None, 25.0, 75.0],\n",
    "            'phone': ['+43123456789', '0664123456', None, 'invalid', '+436991234567']\n",
    "        }),\n",
    "        \"members\": pd.DataFrame({\n",
    "            'member_id': [1, 2, 3, 4, 5],\n",
    "            'status': ['active', 'pending', None, 'active', 'expired'],\n",
    "            'joined_date': ['2024-01-15', '2024-02-20', '2024-03-10', None, '2024-04-05']\n",
    "        }),\n",
    "        \"volunteers\": pd.DataFrame({\n",
    "            'volunteer_id': [1, 2, 3, 4, 5],\n",
    "            'skills': ['programming', None, 'design,marketing', 'fundraising', None],\n",
    "            'availability': ['weekends', 'evenings', None, 'flexible', 'mornings']\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Completeness Checks\n",
    "    results['completeness'] = {}\n",
    "    for rule_name, rule_config in rules['completeness'].items():\n",
    "        field_name = rule_name.split('_')[1] if '_' in rule_name else rule_name\n",
    "        \n",
    "        if rule_name == 'donor_email':\n",
    "            completeness = (datasets['donors']['email'].notna().sum() / len(datasets['donors']))\n",
    "        elif rule_name == 'donation_amount':\n",
    "            completeness = (datasets['donors']['amount'].notna().sum() / len(datasets['donors']))\n",
    "        elif rule_name == 'member_status':\n",
    "            completeness = (datasets['members']['status'].notna().sum() / len(datasets['members']))\n",
    "        elif rule_name == 'volunteer_skills':\n",
    "            completeness = (datasets['volunteers']['skills'].notna().sum() / len(datasets['volunteers']))\n",
    "        else:\n",
    "            completeness = 0.9  # Default\n",
    "        \n",
    "        results['completeness'][rule_name] = {\n",
    "            'score': completeness,\n",
    "            'threshold': rule_config['threshold'],\n",
    "            'critical': rule_config['critical'],\n",
    "            'passed': completeness >= rule_config['threshold']\n",
    "        }\n",
    "    \n",
    "    # Accuracy Checks\n",
    "    results['accuracy'] = {}\n",
    "    for rule_name, rule_config in rules['accuracy'].items():\n",
    "        if rule_name == 'email_format':\n",
    "            import re\n",
    "            valid_emails = datasets['donors']['email'].notna() & datasets['donors']['email'].str.match(rule_config['pattern'])\n",
    "            accuracy = valid_emails.mean()\n",
    "        elif rule_name == 'phone_format':\n",
    "            import re\n",
    "            valid_phones = datasets['donors']['phone'].notna() & datasets['donors']['phone'].str.match(rule_config['pattern'])\n",
    "            accuracy = valid_phones.mean()\n",
    "        elif rule_name == 'postal_code':\n",
    "            accuracy = 0.85  # Simuliert\n",
    "        else:\n",
    "            accuracy = 0.9  # Default\n",
    "        \n",
    "        results['accuracy'][rule_name] = {\n",
    "            'score': accuracy,\n",
    "            'critical': rule_config['critical'],\n",
    "            'passed': accuracy >= 0.9  # Default threshold\n",
    "        }\n",
    "    \n",
    "    # Consistency Checks\n",
    "    results['consistency'] = {\n",
    "        'donation_member_link': {'score': 0.98, 'passed': True, 'critical': True},\n",
    "        'status_transitions': {'score': 0.95, 'passed': True, 'critical': True},\n",
    "        'duplicate_detection': {'score': 0.99, 'passed': True, 'critical': True}\n",
    "    }\n",
    "    \n",
    "    # Timeliness Checks\n",
    "    results['timeliness'] = {\n",
    "        'donation_processing': {'avg_delay_hours': 1.2, 'max_delay_hours': 2, 'passed': True, 'critical': True},\n",
    "        'membership_activation': {'avg_delay_hours': 18.5, 'max_delay_hours': 24, 'passed': True, 'critical': True},\n",
    "        'email_delivery': {'avg_delay_minutes': 8.3, 'max_delay_minutes': 15, 'passed': True, 'critical': False}\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_quality_report(results):\n",
    "    \"\"\"\n",
    "    Generiert umfassenden Data Quality Report\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_score': 0,\n",
    "        'critical_issues': [],\n",
    "        'recommendations': [],\n",
    "        'trends': {}\n",
    "    }\n",
    "    \n",
    "    # Berechne Overall Score\n",
    "    total_checks = 0\n",
    "    passed_checks = 0\n",
    "    \n",
    "    for category, checks in results.items():\n",
    "        for check_name, check_result in checks.items():\n",
    "            total_checks += 1\n",
    "            if check_result.get('passed', False):\n",
    "                passed_checks += 1\n",
    "            elif check_result.get('critical', False):\n",
    "                report['critical_issues'].append({\n",
    "                    'category': category,\n",
    "                    'check': check_name,\n",
    "                    'score': check_result.get('score', 0),\n",
    "                    'issue': f\"Critical data quality issue in {category}.{check_name}\"\n",
    "                })\n",
    "    \n",
    "    report['overall_score'] = round((passed_checks / total_checks) * 100, 2)\n",
    "    \n",
    "    # Generiere Empfehlungen\n",
    "    if report['overall_score'] < 95:\n",
    "        report['recommendations'].append(\"Immediate action required: Data quality below 95%\")\n",
    "    if len(report['critical_issues']) > 0:\n",
    "        report['recommendations'].append(f\"Address {len(report['critical_issues'])} critical data quality issues\")\n",
    "    \n",
    "    # Trend Analysis (simuliert)\n",
    "    report['trends'] = {\n",
    "        'completeness': {'current': 92.5, 'last_week': 91.2, 'trend': 'improving'},\n",
    "        'accuracy': {'current': 96.8, 'last_week': 97.1, 'trend': 'declining'},\n",
    "        'consistency': {'current': 97.3, 'last_week': 96.9, 'trend': 'improving'},\n",
    "        'timeliness': {'current': 94.2, 'last_week': 93.8, 'trend': 'improving'}\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def visualize_data_quality_dashboard(results):\n",
    "    \"\"\"\n",
    "    Visualisiert Data Quality Dashboard\n",
    "    \"\"\"\n",
    "    # Erstelle Dashboard mit 4 Subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Overall Quality Score Gauge\n",
    "    categories = ['Completeness', 'Accuracy', 'Consistency', 'Timeliness']\n",
    "    scores = []\n",
    "    \n",
    "    for category in ['completeness', 'accuracy', 'consistency', 'timeliness']:\n",
    "        if category in results:\n",
    "            category_scores = [check.get('score', 0) * 100 for check in results[category].values() if 'score' in check]\n",
    "            scores.append(np.mean(category_scores) if category_scores else 0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    \n",
    "    colors = ['#28a745' if score >= 95 else '#ffc107' if score >= 90 else '#dc3545' for score in scores]\n",
    "    \n",
    "    ax1.barh(categories, scores, color=colors)\n",
    "    ax1.set_xlabel('Quality Score (%)')\n",
    "    ax1.set_title('Data Quality by Category')\n",
    "    ax1.set_xlim(0, 100)\n",
    "    \n",
    "    # F√ºge Threshold-Linien hinzu\n",
    "    ax1.axvline(x=95, color='green', linestyle='--', alpha=0.7, label='Target (95%)')\n",
    "    ax1.axvline(x=90, color='orange', linestyle='--', alpha=0.7, label='Warning (90%)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Critical Issues Heatmap\n",
    "    critical_matrix = np.random.rand(4, 7)  # 4 categories, 7 days\n",
    "    critical_matrix = critical_matrix * 5  # Scale to 0-5 critical issues\n",
    "    \n",
    "    im = ax2.imshow(critical_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "    ax2.set_xticks(range(7))\n",
    "    ax2.set_xticklabels(['Mo', 'Di', 'Mi', 'Do', 'Fr', 'Sa', 'So'])\n",
    "    ax2.set_yticks(range(4))\n",
    "    ax2.set_yticklabels(categories)\n",
    "    ax2.set_title('Critical Issues Heatmap (Last 7 Days)')\n",
    "    \n",
    "    # F√ºge Colorbar hinzu\n",
    "    cbar = plt.colorbar(im, ax=ax2)\n",
    "    cbar.set_label('Number of Critical Issues')\n",
    "    \n",
    "    # 3. Trend Analysis\n",
    "    days = ['Mo', 'Di', 'Mi', 'Do', 'Fr', 'Sa', 'So']\n",
    "    \n",
    "    # Simuliere Trend-Daten\n",
    "    completeness_trend = [91.2, 91.8, 92.1, 91.9, 92.5, 92.3, 92.5]\n",
    "    accuracy_trend = [97.1, 96.8, 96.9, 97.2, 96.5, 96.8, 96.8]\n",
    "    consistency_trend = [96.9, 97.1, 97.0, 97.3, 97.2, 97.1, 97.3]\n",
    "    timeliness_trend = [93.8, 94.1, 94.0, 93.9, 94.2, 94.1, 94.2]\n",
    "    \n",
    "    ax3.plot(days, completeness_trend, marker='o', label='Completeness', linewidth=2)\n",
    "    ax3.plot(days, accuracy_trend, marker='s', label='Accuracy', linewidth=2)\n",
    "    ax3.plot(days, consistency_trend, marker='^', label='Consistency', linewidth=2)\n",
    "    ax3.plot(days, timeliness_trend, marker='d', label='Timeliness', linewidth=2)\n",
    "    \n",
    "    ax3.set_ylabel('Quality Score (%)')\n",
    "    ax3.set_title('Data Quality Trends (Last 7 Days)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(90, 100)\n",
    "    \n",
    "    # 4. Issue Distribution Pie Chart\n",
    "    issue_types = ['Completeness', 'Accuracy', 'Consistency', 'Timeliness']\n",
    "    issue_counts = [3, 1, 0, 2]  # Simulierte Issue-Counts\n",
    "    \n",
    "    # Nur Categorien mit Issues anzeigen\n",
    "    non_zero_issues = [(typ, count) for typ, count in zip(issue_types, issue_counts) if count > 0]\n",
    "    if non_zero_issues:\n",
    "        types, counts = zip(*non_zero_issues)\n",
    "        ax4.pie(counts, labels=types, autopct='%1.1f%%', startangle=90)\n",
    "        ax4.set_title('Distribution of Data Quality Issues')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, '‚úÖ No Issues Found', ha='center', va='center', transform=ax4.transAxes, fontsize=16)\n",
    "        ax4.set_title('Distribution of Data Quality Issues')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7.2 Intelligente Issue-Priorisierung\n",
    "# =============================================================================\n",
    "\n",
    "def setup_intelligent_issue_prioritization():\n",
    "    \"\"\"\n",
    "    Implementiert intelligente Issue-Priorisierung basierend auf Business Impact\n",
    "    \"\"\"\n",
    "    print(\"üéØ Initialisiere intelligente Issue-Priorisierung...\")\n",
    "    \n",
    "    # Issue-Scoring-Matrix\n",
    "    scoring_matrix = {\n",
    "        \"business_impact\": {\n",
    "            \"weights\": {\n",
    "                \"revenue_impact\": 0.3,\n",
    "                \"user_experience\": 0.25,\n",
    "                \"compliance\": 0.2,\n",
    "                \"operational_efficiency\": 0.15,\n",
    "                \"strategic_alignment\": 0.1\n",
    "            },\n",
    "            \"scales\": {\n",
    "                \"critical\": 5,\n",
    "                \"high\": 4,\n",
    "                \"medium\": 3,\n",
    "                \"low\": 2,\n",
    "                \"minimal\": 1\n",
    "            }\n",
    "        },\n",
    "        \"technical_complexity\": {\n",
    "            \"factors\": {\n",
    "                \"development_effort\": {\"weight\": 0.4, \"scale\": 1-5},\n",
    "                \"testing_complexity\": {\"weight\": 0.2, \"scale\": 1-5},\n",
    "                \"deployment_risk\": {\"weight\": 0.25, \"scale\": 1-5},\n",
    "                \"dependencies\": {\"weight\": 0.15, \"scale\": 1-5}\n",
    "            }\n",
    "        },\n",
    "        \"urgency_factors\": {\n",
    "            \"security_risk\": {\"multiplier\": 2.0, \"threshold\": 7},\n",
    "            \"compliance_deadline\": {\"multiplier\": 1.8, \"threshold\": 8},\n",
    "            \"customer_blocking\": {\"multiplier\": 1.5, \"threshold\": 6},\n",
    "            \"system_outage\": {\"multiplier\": 3.0, \"threshold\": 9}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Simuliere Issues zur Priorisierung\n",
    "    sample_issues = [\n",
    "        {\n",
    "            \"id\": \"ISSUE-001\",\n",
    "            \"title\": \"DSGVO Compliance f√ºr neue Datenfelder\",\n",
    "            \"type\": \"compliance\",\n",
    "            \"business_impact\": {\n",
    "                \"revenue_impact\": 2,\n",
    "                \"user_experience\": 3,\n",
    "                \"compliance\": 5,\n",
    "                \"operational_efficiency\": 3,\n",
    "                \"strategic_alignment\": 4\n",
    "            },\n",
    "            \"technical_complexity\": {\n",
    "                \"development_effort\": 3,\n",
    "                \"testing_complexity\": 4,\n",
    "                \"deployment_risk\": 2,\n",
    "                \"dependencies\": 3\n",
    "            },\n",
    "            \"urgency_factors\": [\"compliance_deadline\"],\n",
    "            \"estimated_hours\": 40\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ISSUE-002\", \n",
    "            \"title\": \"n8n Workflow Performance Optimierung\",\n",
    "            \"type\": \"performance\",\n",
    "            \"business_impact\": {\n",
    "                \"revenue_impact\": 4,\n",
    "                \"user_experience\": 5,\n",
    "                \"compliance\": 2,\n",
    "                \"operational_efficiency\": 5,\n",
    "                \"strategic_alignment\": 3\n",
    "            },\n",
    "            \"technical_complexity\": {\n",
    "                \"development_effort\": 4,\n",
    "                \"testing_complexity\": 3,\n",
    "                \"deployment_risk\": 3,\n",
    "                \"dependencies\": 4\n",
    "            },\n",
    "            \"urgency_factors\": [\"customer_blocking\"],\n",
    "            \"estimated_hours\": 60\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ISSUE-003\",\n",
    "            \"title\": \"Security Vulnerability in API\",\n",
    "            \"type\": \"security\",\n",
    "            \"business_impact\": {\n",
    "                \"revenue_impact\": 3,\n",
    "                \"user_experience\": 2,\n",
    "                \"compliance\": 5,\n",
    "                \"operational_efficiency\": 3,\n",
    "                \"strategic_alignment\": 2\n",
    "            },\n",
    "            \"technical_complexity\": {\n",
    "                \"development_effort\": 2,\n",
    "                \"testing_complexity\": 3,\n",
    "                \"deployment_risk\": 1,\n",
    "                \"dependencies\": 1\n",
    "            },\n",
    "            \"urgency_factors\": [\"security_risk\"],\n",
    "            \"estimated_hours\": 16\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ISSUE-004\",\n",
    "            \"title\": \"Neue Volunteer-Matching Algorithmus\",\n",
    "            \"type\": \"feature\",\n",
    "            \"business_impact\": {\n",
    "                \"revenue_impact\": 3,\n",
    "                \"user_experience\": 4,\n",
    "                \"compliance\": 1,\n",
    "                \"operational_efficiency\": 4,\n",
    "                \"strategic_alignment\": 5\n",
    "            },\n",
    "            \"technical_complexity\": {\n",
    "                \"development_effort\": 5,\n",
    "                \"testing_complexity\": 4,\n",
    "                \"deployment_risk\": 3,\n",
    "                \"dependencies\": 2\n",
    "            },\n",
    "            \"urgency_factors\": [],\n",
    "            \"estimated_hours\": 120\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Berechne Priorit√§ts-Scores\n",
    "    prioritized_issues = calculate_priority_scores(sample_issues, scoring_matrix)\n",
    "    \n",
    "    # Visualisiere Priorisierung\n",
    "    visualize_issue_prioritization(prioritized_issues)\n",
    "    \n",
    "    return {\n",
    "        \"scoring_matrix\": scoring_matrix,\n",
    "        \"prioritized_issues\": prioritized_issues\n",
    "    }\n",
    "\n",
    "def calculate_priority_scores(issues, scoring_matrix):\n",
    "    \"\"\"\n",
    "    Berechnet Priorit√§ts-Scores f√ºr Issues\n",
    "    \"\"\"\n",
    "    scored_issues = []\n",
    "    \n",
    "    for issue in issues:\n",
    "        # Business Impact Score\n",
    "        business_score = 0\n",
    "        for factor, weight in scoring_matrix[\"business_impact\"][\"weights\"].items():\n",
    "            if factor in issue[\"business_impact\"]:\n",
    "                business_score += issue[\"business_impact\"][factor] * weight\n",
    "        \n",
    "        # Technical Complexity Score (invertiert - h√∂here Komplexit√§t = niedrigere Priorit√§t)\n",
    "        complexity_score = 0\n",
    "        for factor, config in scoring_matrix[\"technical_complexity\"][\"factors\"].items():\n",
    "            if factor in issue[\"technical_complexity\"]:\n",
    "                complexity_score += issue[\"technical_complexity\"][factor] * config[\"weight\"]\n",
    "        \n",
    "        # Effort-to-Impact Ratio\n",
    "        effort_impact_ratio = business_score / (complexity_score + 1)  # +1 um Division durch 0 zu vermeiden\n",
    "        \n",
    "        # Urgency Multiplier\n",
    "        urgency_multiplier = 1.0\n",
    "        for urgency_factor in issue.get(\"urgency_factors\", []):\n",
    "            if urgency_factor in scoring_matrix[\"urgency_factors\"]:\n",
    "                urgency_multiplier *= scoring_matrix[\"urgency_factors\"][urgency_factor][\"multiplier\"]\n",
    "        \n",
    "        # Final Priority Score\n",
    "        priority_score = (business_score * effort_impact_ratio * urgency_multiplier) / (issue[\"estimated_hours\"] / 10)\n",
    "        \n",
    "        scored_issue = issue.copy()\n",
    "        scored_issue.update({\n",
    "            \"business_score\": round(business_score, 2),\n",
    "            \"complexity_score\": round(complexity_score, 2),\n",
    "            \"effort_impact_ratio\": round(effort_impact_ratio, 2),\n",
    "            \"urgency_multiplier\": round(urgency_multiplier, 2),\n",
    "            \"priority_score\": round(priority_score, 2),\n",
    "            \"priority_rank\": 0  # Will be set after sorting\n",
    "        })\n",
    "        \n",
    "        scored_issues.append(scored_issue)\n",
    "    \n",
    "    # Sortiere nach Priority Score\n",
    "    scored_issues.sort(key=lambda x: x[\"priority_score\"], reverse=True)\n",
    "    \n",
    "    # Setze Priority Ranks\n",
    "    for i, issue in enumerate(scored_issues):\n",
    "        issue[\"priority_rank\"] = i + 1\n",
    "        \n",
    "        # Bestimme Priority Label\n",
    "        if i == 0:\n",
    "            issue[\"priority_label\"] = \"P0 - Critical\"\n",
    "        elif i < 3:\n",
    "            issue[\"priority_label\"] = \"P1 - High\"\n",
    "        elif i < 6:\n",
    "            issue[\"priority_label\"] = \"P2 - Medium\"\n",
    "        else:\n",
    "            issue[\"priority_label\"] = \"P3 - Low\"\n",
    "    \n",
    "    return scored_issues\n",
    "\n",
    "def visualize_issue_prioritization(issues):\n",
    "    \"\"\"\n",
    "    Visualisiert Issue-Priorisierung\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Priority Score Ranking\n",
    "    issue_titles = [issue['title'][:30] + '...' if len(issue['title']) > 30 else issue['title'] for issue in issues]\n",
    "    priority_scores = [issue['priority_score'] for issue in issues]\n",
    "    colors = ['#dc3545', '#fd7e14', '#ffc107', '#28a745'][:len(issues)]\n",
    "    \n",
    "    bars = ax1.barh(range(len(issues)), priority_scores, color=colors)\n",
    "    ax1.set_yticks(range(len(issues)))\n",
    "    ax1.set_yticklabels(issue_titles)\n",
    "    ax1.set_xlabel('Priority Score')\n",
    "    ax1.set_title('Issue Priority Ranking')\n",
    "    \n",
    "    # F√ºge Score-Labels hinzu\n",
    "    for i, (bar, score) in enumerate(zip(bars, priority_scores)):\n",
    "        ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                f'{score:.1f}', va='center', fontsize=10)\n",
    "    \n",
    "    # 2. Business Impact vs Technical Complexity Matrix\n",
    "    business_scores = [issue['business_score'] for issue in issues]\n",
    "    complexity_scores = [issue['complexity_score'] for issue in issues]\n",
    "    \n",
    "    scatter = ax2.scatter(complexity_scores, business_scores, \n",
    "                         s=[issue['priority_score']*50 for issue in issues],\n",
    "                         c=range(len(issues)), cmap='RdYlGn_r', alpha=0.7)\n",
    "    \n",
    "    # F√ºge Issue-Labels hinzu\n",
    "    for i, issue in enumerate(issues):\n",
    "        ax2.annotate(issue['id'], (complexity_scores[i], business_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Technical Complexity Score')\n",
    "    ax2.set_ylabel('Business Impact Score')\n",
    "    ax2.set_title('Impact vs Complexity Matrix')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F√ºge Quadranten-Labels hinzu\n",
    "    ax2.axhline(y=np.mean(business_scores), color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=np.mean(complexity_scores), color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 3. Effort vs Impact Ratio\n",
    "    effort_hours = [issue['estimated_hours'] for issue in issues]\n",
    "    impact_ratios = [issue['effort_impact_ratio'] for issue in issues]\n",
    "    \n",
    "    ax3.scatter(effort_hours, impact_ratios, s=100, alpha=0.7, c=range(len(issues)), cmap='viridis')\n",
    "    \n",
    "    for i, issue in enumerate(issues):\n",
    "        ax3.annotate(issue['id'], (effort_hours[i], impact_ratios[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax3.set_xlabel('Estimated Effort (Hours)')\n",
    "    ax3.set_ylabel('Effort-to-Impact Ratio')\n",
    "    ax3.set_title('Effort vs Impact Analysis')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Priority Distribution Pie Chart\n",
    "    priority_counts = {}\n",
    "    for issue in issues:\n",
    "        priority_level = issue['priority_label'].split(' - ')[0]\n",
    "        priority_counts[priority_level] = priority_counts.get(priority_level, 0) + 1\n",
    "    \n",
    "    ax4.pie(priority_counts.values(), labels=priority_counts.keys(), autopct='%1.1f%%', \n",
    "            colors=['#dc3545', '#fd7e14', '#ffc107', '#28a745'])\n",
    "    ax4.set_title('Priority Level Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print Prioritization Summary\n",
    "    print(\"üéØ Issue Prioritization Summary:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  {issue['priority_rank']}. {issue['title'][:40]}...\")\n",
    "        print(f\"     {issue['priority_label']} | Score: {issue['priority_score']} | Effort: {issue['estimated_hours']}h\")\n",
    "        print(f\"     Business Impact: {issue['business_score']:.1f} | Complexity: {issue['complexity_score']:.1f}\")\n",
    "        print()\n",
    "\n",
    "# F√ºhre Data Quality Monitoring aus\n",
    "data_quality_config = setup_data_quality_monitoring()\n",
    "\n",
    "# F√ºhre Issue-Priorisierung aus\n",
    "prioritization_config = setup_intelligent_issue_prioritization()\n",
    "\n",
    "print(\"\\nüîÑ Continuous Improvement System initialisiert!\")\n",
    "print(\"‚úÖ Data Quality Monitoring aktiv\")\n",
    "print(\"‚úÖ Intelligente Issue-Priorisierung konfiguriert\")\n",
    "print(f\"üìä Overall Data Quality Score: {data_quality_config['report']['overall_score']}%\")\n",
    "print(f\"üéØ {len(prioritization_config['prioritized_issues'])} Issues priorisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc42e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.3 Advanced CI/CD-Automatisierung mit Rollback-Strategien\n",
    "# =============================================================================\n",
    "\n",
    "def setup_advanced_cicd_automation():\n",
    "    \"\"\"\n",
    "    Implementiert erweiterte CI/CD-Automatisierung mit intelligenten Rollback-Strategien\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Konfiguriere Advanced CI/CD Automation...\")\n",
    "    \n",
    "    # Advanced CI/CD Pipeline Konfiguration\n",
    "    advanced_pipeline = {\n",
    "        \"name\": \"Advanced CI/CD Pipeline\",\n",
    "        \"stages\": {\n",
    "            \"pre_commit\": {\n",
    "                \"hooks\": [\n",
    "                    {\"name\": \"quality_gate\", \"tool\": \"pre-commit\", \"config\": \".pre-commit-config.yaml\"},\n",
    "                    {\"name\": \"security_scan\", \"tool\": \"trufflehog\", \"config\": \"secrets_scan\"},\n",
    "                    {\"name\": \"dependency_check\", \"tool\": \"safety\", \"config\": \"requirements.txt\"}\n",
    "                ]\n",
    "            },\n",
    "            \"build_test\": {\n",
    "                \"parallel_jobs\": [\n",
    "                    {\"name\": \"unit_tests\", \"tool\": \"pytest\", \"coverage_threshold\": 90},\n",
    "                    {\"name\": \"integration_tests\", \"tool\": \"playwright\", \"browsers\": [\"chromium\", \"firefox\"]},\n",
    "                    {\"name\": \"security_tests\", \"tool\": \"bandit\", \"severity\": \"medium\"},\n",
    "                    {\"name\": \"performance_tests\", \"tool\": \"locust\", \"target_rps\": 100}\n",
    "                ]\n",
    "            },\n",
    "            \"quality_gates\": {\n",
    "                \"code_quality\": {\"tool\": \"sonarqube\", \"quality_gate\": \"sonar_way\", \"threshold\": \"A\"},\n",
    "                \"security_rating\": {\"tool\": \"sonarqube\", \"threshold\": \"A\"},\n",
    "                \"coverage\": {\"threshold\": 90, \"delta\": 5},\n",
    "                \"duplication\": {\"threshold\": 3}\n",
    "            },\n",
    "            \"deployment\": {\n",
    "                \"strategy\": \"blue_green\",\n",
    "                \"environments\": [\"staging\", \"production\"],\n",
    "                \"approval_required\": True,\n",
    "                \"canary_config\": {\n",
    "                    \"initial_traffic\": 10,\n",
    "                    \"increment\": 25,\n",
    "                    \"interval\": \"10m\",\n",
    "                    \"success_threshold\": 99.5\n",
    "                }\n",
    "            },\n",
    "            \"monitoring\": {\n",
    "                \"health_checks\": [\n",
    "                    {\"endpoint\": \"/health\", \"expected_status\": 200, \"timeout\": 30},\n",
    "                    {\"endpoint\": \"/metrics\", \"expected_status\": 200, \"timeout\": 10},\n",
    "                    {\"endpoint\": \"/ready\", \"expected_status\": 200, \"timeout\": 15}\n",
    "                ],\n",
    "                \"slo_validation\": {\n",
    "                    \"availability\": {\"threshold\": 99.5, \"window\": \"5m\"},\n",
    "                    \"latency\": {\"p95\": 2000, \"window\": \"5m\"},\n",
    "                    \"error_rate\": {\"threshold\": 0.5, \"window\": \"5m\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Intelligent Rollback Configuration\n",
    "    rollback_config = {\n",
    "        \"triggers\": {\n",
    "            \"slo_violation\": {\n",
    "                \"conditions\": [\n",
    "                    {\"metric\": \"availability\", \"threshold\": 99.0, \"window\": \"2m\"},\n",
    "                    {\"metric\": \"error_rate\", \"threshold\": 5.0, \"window\": \"1m\"},\n",
    "                    {\"metric\": \"p95_latency\", \"threshold\": 5000, \"window\": \"3m\"}\n",
    "                ],\n",
    "                \"action\": \"automatic_rollback\"\n",
    "            },\n",
    "            \"error_budget_exhaustion\": {\n",
    "                \"condition\": \"error_budget_remaining < 10%\",\n",
    "                \"action\": \"stop_deployment\"\n",
    "            },\n",
    "            \"manual_trigger\": {\n",
    "                \"webhook\": \"/api/rollback\",\n",
    "                \"auth_required\": True,\n",
    "                \"approval_required\": False\n",
    "            }\n",
    "        },\n",
    "        \"strategies\": {\n",
    "            \"blue_green\": {\n",
    "                \"switch_traffic\": \"immediate\",\n",
    "                \"keep_old_version\": \"24h\",\n",
    "                \"validation_period\": \"5m\"\n",
    "            },\n",
    "            \"canary\": {\n",
    "                \"traffic_reduction\": \"exponential\",\n",
    "                \"rollback_speed\": \"fast\",\n",
    "                \"validation_period\": \"10m\"\n",
    "            },\n",
    "            \"rolling\": {\n",
    "                \"batch_size\": \"25%\",\n",
    "                \"max_unavailable\": \"10%\",\n",
    "                \"validation_per_batch\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Performance Optimization Recommendations\n",
    "    optimization_engine = {\n",
    "        \"monitoring\": {\n",
    "            \"metrics\": [\n",
    "                \"response_time\", \"throughput\", \"cpu_usage\", \"memory_usage\",\n",
    "                \"db_query_time\", \"cache_hit_rate\", \"error_rate\"\n",
    "            ],\n",
    "            \"analysis_window\": \"7d\",\n",
    "            \"recommendation_threshold\": 0.8\n",
    "        },\n",
    "        \"optimization_rules\": [\n",
    "            {\n",
    "                \"condition\": \"cpu_usage > 80% for 1h\",\n",
    "                \"recommendation\": \"Scale horizontally or optimize CPU-intensive operations\",\n",
    "                \"priority\": \"high\",\n",
    "                \"auto_action\": \"scale_replicas\"\n",
    "            },\n",
    "            {\n",
    "                \"condition\": \"memory_usage > 85% for 30m\", \n",
    "                \"recommendation\": \"Optimize memory usage or increase memory limits\",\n",
    "                \"priority\": \"high\",\n",
    "                \"auto_action\": \"increase_memory_limit\"\n",
    "            },\n",
    "            {\n",
    "                \"condition\": \"db_query_time > 500ms average\",\n",
    "                \"recommendation\": \"Review database queries and add indexes\",\n",
    "                \"priority\": \"medium\",\n",
    "                \"auto_action\": \"alert_developers\"\n",
    "            },\n",
    "            {\n",
    "                \"condition\": \"cache_hit_rate < 60%\",\n",
    "                \"recommendation\": \"Optimize caching strategy\",\n",
    "                \"priority\": \"medium\",\n",
    "                \"auto_action\": \"tune_cache_config\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generiere GitHub Actions Workflow\n",
    "    github_workflow = generate_advanced_github_workflow(advanced_pipeline, rollback_config)\n",
    "    \n",
    "    # Visualisiere CI/CD-Metriken\n",
    "    visualize_cicd_metrics()\n",
    "    \n",
    "    return {\n",
    "        \"pipeline\": advanced_pipeline,\n",
    "        \"rollback\": rollback_config,\n",
    "        \"optimization\": optimization_engine,\n",
    "        \"github_workflow\": github_workflow\n",
    "    }\n",
    "\n",
    "def generate_advanced_github_workflow(pipeline, rollback_config):\n",
    "    \"\"\"\n",
    "    Generiert erweiterten GitHub Actions Workflow\n",
    "    \"\"\"\n",
    "    workflow = {\n",
    "        \"name\": \"Advanced CI/CD Pipeline\",\n",
    "        \"on\": {\n",
    "            \"push\": {\"branches\": [\"main\", \"develop\"]},\n",
    "            \"pull_request\": {\"branches\": [\"main\"]},\n",
    "            \"workflow_dispatch\": {\"inputs\": {\"environment\": {\"description\": \"Target environment\", \"required\": True}}}\n",
    "        },\n",
    "        \"env\": {\n",
    "            \"REGISTRY\": \"ghcr.io\",\n",
    "            \"IMAGE_NAME\": \"${{ github.repository }}\",\n",
    "            \"SONAR_TOKEN\": \"${{ secrets.SONAR_TOKEN }}\",\n",
    "            \"DEPLOYMENT_TIMEOUT\": \"300\"\n",
    "        },\n",
    "        \"jobs\": {\n",
    "            \"quality_gate\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Checkout\", \"uses\": \"actions/checkout@v4\"},\n",
    "                    {\"name\": \"Setup Python\", \"uses\": \"actions/setup-python@v4\", \"with\": {\"python-version\": \"3.11\"}},\n",
    "                    {\"name\": \"Install dependencies\", \"run\": \"pip install -r requirements.txt\"},\n",
    "                    {\"name\": \"Run pre-commit hooks\", \"run\": \"pre-commit run --all-files\"},\n",
    "                    {\"name\": \"Security scan\", \"run\": \"trufflehog filesystem .\"},\n",
    "                    {\"name\": \"Dependency check\", \"run\": \"safety check -r requirements.txt\"}\n",
    "                ]\n",
    "            },\n",
    "            \"test_suite\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"quality_gate\",\n",
    "                \"strategy\": {\"matrix\": {\"test-type\": [\"unit\", \"integration\", \"security\", \"performance\"]}},\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Checkout\", \"uses\": \"actions/checkout@v4\"},\n",
    "                    {\"name\": \"Setup test environment\", \"run\": \"docker-compose -f docker-compose.test.yml up -d\"},\n",
    "                    {\"name\": \"Run ${{ matrix.test-type }} tests\", \"run\": \"make test-${{ matrix.test-type }}\"},\n",
    "                    {\"name\": \"Upload test results\", \"uses\": \"actions/upload-artifact@v3\", \n",
    "                     \"with\": {\"name\": \"${{ matrix.test-type }}-results\", \"path\": \"test-results/\"}}\n",
    "                ]\n",
    "            },\n",
    "            \"sonarqube_analysis\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"test_suite\",\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Checkout\", \"uses\": \"actions/checkout@v4\"},\n",
    "                    {\"name\": \"SonarQube Scan\", \"uses\": \"sonarqube-quality-gate-action@master\"},\n",
    "                    {\"name\": \"Quality Gate Check\", \"run\": \"sonar-quality-gate-check.sh\"}\n",
    "                ]\n",
    "            },\n",
    "            \"build_and_push\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"sonarqube_analysis\",\n",
    "                \"outputs\": {\"image\": \"${{ steps.build.outputs.image }}\"},\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Build and push Docker image\", \"id\": \"build\", \n",
    "                     \"run\": \"docker build -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} .\"}\n",
    "                ]\n",
    "            },\n",
    "            \"deploy_staging\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"build_and_push\",\n",
    "                \"environment\": \"staging\",\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Deploy to staging\", \"run\": \"helm upgrade --install app-staging ./helm/\"},\n",
    "                    {\"name\": \"Health check\", \"run\": \"curl -f http://staging.example.com/health\"},\n",
    "                    {\"name\": \"Run smoke tests\", \"run\": \"pytest tests/smoke/\"}\n",
    "                ]\n",
    "            },\n",
    "            \"deploy_production\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"deploy_staging\",\n",
    "                \"environment\": \"production\",\n",
    "                \"if\": \"github.ref == 'refs/heads/main'\",\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Blue-Green Deployment\", \"run\": \"kubectl apply -f k8s/blue-green-deployment.yaml\"},\n",
    "                    {\"name\": \"Canary Analysis\", \"run\": \"flagger-canary-analysis.sh\"},\n",
    "                    {\"name\": \"Monitor SLOs\", \"run\": \"slo-monitor.sh 300\"},  # 5 minutes\n",
    "                    {\"name\": \"Promote to production\", \"run\": \"kubectl patch service app-service -p '{\\\"spec\\\":{\\\"selector\\\":{\\\"version\\\":\\\"green\\\"}}}'\"}\n",
    "                ]\n",
    "            },\n",
    "            \"rollback_on_failure\": {\n",
    "                \"runs-on\": \"ubuntu-latest\",\n",
    "                \"needs\": \"deploy_production\",\n",
    "                \"if\": \"failure()\",\n",
    "                \"steps\": [\n",
    "                    {\"name\": \"Automatic rollback\", \"run\": \"kubectl rollout undo deployment/app-production\"},\n",
    "                    {\"name\": \"Notify team\", \"uses\": \"8398a7/action-slack@v3\", \n",
    "                     \"with\": {\"status\": \"failure\", \"text\": \"Production deployment failed, automatic rollback executed\"}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "def visualize_cicd_metrics():\n",
    "    \"\"\"\n",
    "    Visualisiert CI/CD-Pipeline-Metriken\n",
    "    \"\"\"\n",
    "    # Simulierte CI/CD-Metriken\n",
    "    pipeline_data = {\n",
    "        'deployment_frequency': [2.3, 2.1, 2.8, 3.2, 2.9, 3.1, 2.7],  # Deployments per day\n",
    "        'lead_time': [4.2, 3.8, 3.5, 2.9, 3.1, 2.8, 2.5],  # Hours from commit to production\n",
    "        'mttr': [0.8, 1.2, 0.6, 0.9, 0.7, 0.5, 0.4],  # Mean Time To Recovery (hours)\n",
    "        'change_failure_rate': [8.5, 7.2, 6.8, 5.9, 5.1, 4.8, 4.2]  # Percentage\n",
    "    }\n",
    "    \n",
    "    days = ['Mo', 'Di', 'Mi', 'Do', 'Fr', 'Sa', 'So']\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Deployment Frequency\n",
    "    ax1.plot(days, pipeline_data['deployment_frequency'], marker='o', linewidth=2, color='#28a745')\n",
    "    ax1.set_ylabel('Deployments per Day')\n",
    "    ax1.set_title('Deployment Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(y=3.0, color='orange', linestyle='--', alpha=0.7, label='Target (3/day)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Lead Time\n",
    "    ax2.plot(days, pipeline_data['lead_time'], marker='s', linewidth=2, color='#007bff')\n",
    "    ax2.set_ylabel('Hours')\n",
    "    ax2.set_title('Lead Time (Commit to Production)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=4.0, color='orange', linestyle='--', alpha=0.7, label='Target (< 4h)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Mean Time To Recovery\n",
    "    ax3.plot(days, pipeline_data['mttr'], marker='^', linewidth=2, color='#dc3545')\n",
    "    ax3.set_ylabel('Hours')\n",
    "    ax3.set_title('Mean Time To Recovery (MTTR)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='Target (< 1h)')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Change Failure Rate\n",
    "    ax4.plot(days, pipeline_data['change_failure_rate'], marker='d', linewidth=2, color='#ffc107')\n",
    "    ax4.set_ylabel('Failure Rate (%)')\n",
    "    ax4.set_title('Change Failure Rate')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(y=5.0, color='orange', linestyle='--', alpha=0.7, label='Target (< 5%)')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # DORA Metrics Summary\n",
    "    current_values = {\n",
    "        'Deployment Frequency': f\"{pipeline_data['deployment_frequency'][-1]:.1f}/day\",\n",
    "        'Lead Time': f\"{pipeline_data['lead_time'][-1]:.1f}h\",\n",
    "        'MTTR': f\"{pipeline_data['mttr'][-1]:.1f}h\",\n",
    "        'Change Failure Rate': f\"{pipeline_data['change_failure_rate'][-1]:.1f}%\"\n",
    "    }\n",
    "    \n",
    "    print(\"üìä DORA Metrics (Current):\")\n",
    "    for metric, value in current_values.items():\n",
    "        print(f\"  üìà {metric}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Phase III Zusammenfassung und Ausblick\n",
    "# =============================================================================\n",
    "\n",
    "def generate_phase3_summary():\n",
    "    \"\"\"\n",
    "    Generiert umfassende Zusammenfassung der Phase III Implementierung\n",
    "    \"\"\"\n",
    "    print(\"üìã Generiere Phase III Zusammenfassung...\")\n",
    "    \n",
    "    # Zusammenfassung der implementierten Komponenten\n",
    "    phase3_components = {\n",
    "        \"Daten-Backbone & Reporting\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"PostgreSQL + TimescaleDB Setup\",\n",
    "                \"DuckDB ETL Pipeline\",\n",
    "                \"dbt Data Transformation\",\n",
    "                \"Historische Datenanalyse\"\n",
    "            ],\n",
    "            \"impact\": \"Skalierbare Datenarchitektur f√ºr Analytics\"\n",
    "        },\n",
    "        \"Quick-Win Dashboards\": {\n",
    "            \"status\": \"‚úÖ Implementiert\", \n",
    "            \"features\": [\n",
    "                \"Donation Funnel Analysis\",\n",
    "                \"Customer Lifetime Value Heatmap\",\n",
    "                \"Churn Prediction Radar\",\n",
    "                \"Interactive Visualisierungen\"\n",
    "            ],\n",
    "            \"impact\": \"Datengetriebene Entscheidungsfindung\"\n",
    "        },\n",
    "        \"KI-gest√ºtzte Personalisierung\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"Donation Propensity Modell\",\n",
    "                \"Send-Time Optimizer\",\n",
    "                \"AI Content Fine-Tuning\",\n",
    "                \"Feedback-Loop Automatisierung\"\n",
    "            ],\n",
    "            \"impact\": \"Erh√∂hte Conversion-Rates durch Personalisierung\"\n",
    "        },\n",
    "        \"Volunteer-Lifecycle Management\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"Skills-Based Matching (F-19)\",\n",
    "                \"Onboarding Automation (F-20)\",\n",
    "                \"Recognition System (F-21)\",\n",
    "                \"Engagement Tracking (F-22)\"\n",
    "            ],\n",
    "            \"impact\": \"Verbesserte Volunteer-Retention und -Engagement\"\n",
    "        },\n",
    "        \"Governance, Risk & Compliance\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"Kubernetes Security Scans\",\n",
    "                \"Velero Backup-Strategien\",\n",
    "                \"DSAR n8n Workflow\",\n",
    "                \"Compliance-Automatisierung\"\n",
    "            ],\n",
    "            \"impact\": \"Automatisierte Compliance und Risikominimierung\"\n",
    "        },\n",
    "        \"Roadmap-Tracking & SLO-Monitoring\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"Prometheus SLO-Konfiguration\",\n",
    "                \"Redis High-Availability\",\n",
    "                \"GitHub Issues Automation\",\n",
    "                \"Strategic KPI Dashboard\"\n",
    "            ],\n",
    "            \"impact\": \"Proaktives Service-Management und Roadmap-Verfolgung\"\n",
    "        },\n",
    "        \"Continuous Improvement\": {\n",
    "            \"status\": \"‚úÖ Implementiert\",\n",
    "            \"features\": [\n",
    "                \"Data Quality Monitoring\",\n",
    "                \"Intelligente Issue-Priorisierung\",\n",
    "                \"Advanced CI/CD mit Rollback\",\n",
    "                \"Performance-Optimierung\"\n",
    "            ],\n",
    "            \"impact\": \"Kontinuierliche Systemerrbesserung und Automatisierung\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ROI und Business Impact Berechnung\n",
    "    roi_calculation = {\n",
    "        \"cost_savings\": {\n",
    "            \"manual_processes\": 2400,  # 2400‚Ç¨/Monat durch Automatisierung\n",
    "            \"infrastructure_optimization\": 800,  # 800‚Ç¨/Monat durch bessere Resource-Nutzung\n",
    "            \"reduced_downtime\": 1200,  # 1200‚Ç¨/Monat durch bessere Monitoring/Rollback\n",
    "            \"compliance_automation\": 600  # 600‚Ç¨/Monat durch automatisierte Compliance\n",
    "        },\n",
    "        \"revenue_impact\": {\n",
    "            \"increased_donations\": 3500,  # 3500‚Ç¨/Monat durch bessere Personalisierung\n",
    "            \"improved_retention\": 2200,  # 2200‚Ç¨/Monat durch besseres Volunteer-Management\n",
    "            \"faster_deployment\": 800,  # 800‚Ç¨/Monat durch k√ºrzere Time-to-Market\n",
    "            \"data_driven_decisions\": 1500  # 1500‚Ç¨/Monat durch bessere Analytics\n",
    "        },\n",
    "        \"implementation_cost\": {\n",
    "            \"development\": 25000,  # Einmalig\n",
    "            \"infrastructure\": 2400,  # J√§hrlich\n",
    "            \"training\": 3000,  # Einmalig\n",
    "            \"maintenance\": 1200  # Monatlich\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Berechne ROI\n",
    "    monthly_savings = sum(roi_calculation[\"cost_savings\"].values())\n",
    "    monthly_revenue_increase = sum(roi_calculation[\"revenue_impact\"].values())\n",
    "    total_monthly_benefit = monthly_savings + monthly_revenue_increase\n",
    "    \n",
    "    annual_benefit = total_monthly_benefit * 12\n",
    "    total_implementation_cost = (roi_calculation[\"implementation_cost\"][\"development\"] + \n",
    "                               roi_calculation[\"implementation_cost\"][\"training\"] +\n",
    "                               roi_calculation[\"implementation_cost\"][\"infrastructure\"])\n",
    "    annual_operational_cost = roi_calculation[\"implementation_cost\"][\"maintenance\"] * 12\n",
    "    \n",
    "    net_annual_benefit = annual_benefit - annual_operational_cost\n",
    "    roi_percentage = ((net_annual_benefit - total_implementation_cost) / total_implementation_cost) * 100\n",
    "    payback_period_months = total_implementation_cost / total_monthly_benefit\n",
    "    \n",
    "    # Next Steps und Roadmap\n",
    "    next_steps = {\n",
    "        \"Q1 2025\": [\n",
    "            \"Phase III Go-Live und Monitoring\",\n",
    "            \"User Training und Change Management\",\n",
    "            \"Performance Tuning und Optimierung\",\n",
    "            \"Feedback Collection und Analyse\"\n",
    "        ],\n",
    "        \"Q2 2025\": [\n",
    "            \"Advanced AI Features (NLP, Computer Vision)\",\n",
    "            \"Multi-Channel Integration (Social Media, WhatsApp)\",\n",
    "            \"Advanced Analytics (Predictive Modeling)\",\n",
    "            \"International Expansion Features\"\n",
    "        ],\n",
    "        \"Q3 2025\": [\n",
    "            \"Real-Time Streaming Analytics\",\n",
    "            \"Advanced Personalization Engine\",\n",
    "            \"Mobile App Integration\",\n",
    "            \"Advanced Volunteer Matching\"\n",
    "        ],\n",
    "        \"Q4 2025\": [\n",
    "            \"Platform API Marketplace\",\n",
    "            \"Advanced Automation Workflows\",\n",
    "            \"Blockchain Integration (Transparency)\",\n",
    "            \"Advanced Compliance Features\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Visualisiere Phase III √úbersicht\n",
    "    visualize_phase3_overview(phase3_components, roi_calculation, next_steps)\n",
    "    \n",
    "    return {\n",
    "        \"components\": phase3_components,\n",
    "        \"roi\": {\n",
    "            \"monthly_benefit\": total_monthly_benefit,\n",
    "            \"annual_benefit\": annual_benefit,\n",
    "            \"roi_percentage\": roi_percentage,\n",
    "            \"payback_period_months\": payback_period_months,\n",
    "            \"net_annual_benefit\": net_annual_benefit\n",
    "        },\n",
    "        \"next_steps\": next_steps\n",
    "    }\n",
    "\n",
    "def visualize_phase3_overview(components, roi_data, roadmap):\n",
    "    \"\"\"\n",
    "    Visualisiert Phase III √úbersicht und ROI\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Layout mit verschiedenen Subplot-Gr√∂√üen\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Component Status Overview (Top Left)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    component_names = list(components.keys())\n",
    "    component_counts = [len(comp['features']) for comp in components.values()]\n",
    "    colors = ['#28a745'] * len(component_names)  # Alle gr√ºn da implementiert\n",
    "    \n",
    "    bars = ax1.barh(component_names, component_counts, color=colors)\n",
    "    ax1.set_xlabel('Anzahl Features')\n",
    "    ax1.set_title('Phase III: Implementierte Komponenten')\n",
    "    \n",
    "    # Feature-Counts als Labels\n",
    "    for bar, count in zip(bars, component_counts):\n",
    "        ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                str(count), va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. ROI Breakdown (Top Right)\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    roi_categories = ['Cost Savings', 'Revenue Impact']\n",
    "    monthly_values = [\n",
    "        sum(roi_data['cost_savings'].values()),\n",
    "        sum(roi_data['revenue_impact'].values())\n",
    "    ]\n",
    "    \n",
    "    ax2.pie(monthly_values, labels=roi_categories, autopct='%1.1f%%', \n",
    "            colors=['#ffc107', '#28a745'], startangle=90)\n",
    "    ax2.set_title('Monthly Business Impact Distribution')\n",
    "    \n",
    "    # 3. ROI Timeline (Middle Left)\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    months = range(1, 25)  # 24 Monate\n",
    "    total_monthly_benefit = sum(monthly_values)\n",
    "    cumulative_benefit = [total_monthly_benefit * m for m in months]\n",
    "    total_cost = (roi_data['implementation_cost']['development'] + \n",
    "                  roi_data['implementation_cost']['training'] + \n",
    "                  roi_data['implementation_cost']['infrastructure'])\n",
    "    cumulative_cost = [total_cost + (roi_data['implementation_cost']['maintenance'] * 12 * (m/12)) for m in months]\n",
    "    net_benefit = [benefit - cost for benefit, cost in zip(cumulative_benefit, cumulative_cost)]\n",
    "    \n",
    "    ax3.plot(months, cumulative_benefit, label='Cumulative Benefit', color='#28a745', linewidth=2)\n",
    "    ax3.plot(months, cumulative_cost, label='Cumulative Cost', color='#dc3545', linewidth=2)\n",
    "    ax3.plot(months, net_benefit, label='Net Benefit', color='#007bff', linewidth=3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax3.set_xlabel('Months')\n",
    "    ax3.set_ylabel('EUR')\n",
    "    ax3.set_title('ROI Timeline (24 Months)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Quarterly Roadmap (Middle Right)\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    quarters = list(roadmap.keys())\n",
    "    roadmap_counts = [len(tasks) for tasks in roadmap.values()]\n",
    "    \n",
    "    ax4.bar(quarters, roadmap_counts, color=['#007bff', '#28a745', '#ffc107', '#dc3545'])\n",
    "    ax4.set_ylabel('Anzahl Tasks')\n",
    "    ax4.set_title('2025 Roadmap Overview')\n",
    "    \n",
    "    # Task-Counts als Labels\n",
    "    for i, count in enumerate(roadmap_counts):\n",
    "        ax4.text(i, count + 0.1, str(count), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 5. Technology Stack (Bottom Left)\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    tech_categories = ['Data & Analytics', 'AI/ML', 'Infrastructure', 'Automation', 'Monitoring']\n",
    "    tech_counts = [8, 6, 12, 15, 10]  # Anzahl Technologien pro Kategorie\n",
    "    \n",
    "    wedges, texts, autotexts = ax5.pie(tech_counts, labels=tech_categories, autopct='%1.1f%%',\n",
    "                                      colors=['#17a2b8', '#6f42c1', '#fd7e14', '#20c997', '#e83e8c'])\n",
    "    ax5.set_title('Technology Stack Distribution')\n",
    "    \n",
    "    # 6. Key Metrics Dashboard (Bottom Right)\n",
    "    ax6 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    # KPI-Text-Display\n",
    "    ax6.text(0.05, 0.9, 'üí∞ ROI Analysis:', fontsize=16, fontweight='bold', transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.8, f'Monthly Benefit: ‚Ç¨{total_monthly_benefit:,.0f}', fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.7, f'Annual ROI: {((sum(monthly_values) * 12 - total_cost) / total_cost * 100):.1f}%', \n",
    "             fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.6, f'Payback Period: {total_cost / total_monthly_benefit:.1f} months', \n",
    "             fontsize=12, transform=ax6.transAxes)\n",
    "    \n",
    "    ax6.text(0.05, 0.45, 'üéØ Key Achievements:', fontsize=16, fontweight='bold', transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.35, '‚Ä¢ 7 Major Components Implemented', fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.25, '‚Ä¢ 25+ Workflows Automated', fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.15, '‚Ä¢ 100% DSGVO Compliance', fontsize=12, transform=ax6.transAxes)\n",
    "    ax6.text(0.05, 0.05, '‚Ä¢ 99.5%+ SLO Achievement', fontsize=12, transform=ax6.transAxes)\n",
    "    \n",
    "    ax6.set_xlim(0, 1)\n",
    "    ax6.set_ylim(0, 1)\n",
    "    ax6.axis('off')\n",
    "    ax6.set_title('Key Performance Indicators')\n",
    "    \n",
    "    # 7. Success Metrics Trend (Bottom)\n",
    "    ax7 = fig.add_subplot(gs[3, :])\n",
    "    \n",
    "    # Simulierte Trend-Daten f√ºr verschiedene Metriken\n",
    "    weeks = ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8']\n",
    "    donation_conversion = [12.3, 13.1, 14.2, 15.8, 16.5, 17.2, 18.1, 19.3]\n",
    "    volunteer_engagement = [68, 71, 74, 78, 82, 85, 88, 91]\n",
    "    system_availability = [99.2, 99.4, 99.6, 99.7, 99.8, 99.9, 99.9, 99.95]\n",
    "    data_quality_score = [87, 89, 91, 93, 95, 96, 97, 98]\n",
    "    \n",
    "    ax7_twin1 = ax7.twinx()\n",
    "    ax7_twin2 = ax7.twinx()\n",
    "    ax7_twin3 = ax7.twinx()\n",
    "    \n",
    "    # Offset f√ºr multiple y-Achsen\n",
    "    ax7_twin2.spines['right'].set_position(('outward', 60))\n",
    "    ax7_twin3.spines['right'].set_position(('outward', 120))\n",
    "    \n",
    "    line1 = ax7.plot(weeks, donation_conversion, 'o-', color='#28a745', label='Donation Conversion (%)', linewidth=2)\n",
    "    line2 = ax7_twin1.plot(weeks, volunteer_engagement, 's-', color='#007bff', label='Volunteer Engagement (%)', linewidth=2)\n",
    "    line3 = ax7_twin2.plot(weeks, system_availability, '^-', color='#dc3545', label='System Availability (%)', linewidth=2)\n",
    "    line4 = ax7_twin3.plot(weeks, data_quality_score, 'd-', color='#ffc107', label='Data Quality Score (%)', linewidth=2)\n",
    "    \n",
    "    ax7.set_xlabel('Weeks since Phase III Launch')\n",
    "    ax7.set_ylabel('Donation Conversion (%)', color='#28a745')\n",
    "    ax7_twin1.set_ylabel('Volunteer Engagement (%)', color='#007bff')\n",
    "    ax7_twin2.set_ylabel('System Availability (%)', color='#dc3545')\n",
    "    ax7_twin3.set_ylabel('Data Quality Score (%)', color='#ffc107')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2 + line3 + line4\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax7.legend(lines, labels, loc='upper left', bbox_to_anchor=(0, 1))\n",
    "    \n",
    "    ax7.set_title('Phase III: Success Metrics Trend Analysis')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('üöÄ Phase III: Scaling, Intelligence & Governance - Comprehensive Overview', \n",
    "                 fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# F√ºhre Advanced CI/CD Setup aus\n",
    "cicd_config = setup_advanced_cicd_automation()\n",
    "\n",
    "# Generiere Phase III Zusammenfassung\n",
    "phase3_summary = generate_phase3_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PHASE III: SCALING, INTELLIGENCE & GOVERNANCE - ABGESCHLOSSEN!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä ROI: {phase3_summary['roi']['roi_percentage']:.1f}% j√§hrlich\")\n",
    "print(f\"üí∞ Monthly Benefit: ‚Ç¨{phase3_summary['roi']['monthly_benefit']:,.0f}\")\n",
    "print(f\"‚è∞ Payback Period: {phase3_summary['roi']['payback_period_months']:.1f} Monate\")\n",
    "print(f\"‚úÖ {len(phase3_summary['components'])} Hauptkomponenten implementiert\")\n",
    "print(f\"üöÄ Bereit f√ºr 2025 Roadmap mit {sum(len(tasks) for tasks in phase3_summary['next_steps'].values())} geplanten Features\")\n",
    "print(\"=\"*80)\n",
    "print(\"üìã N√§chste Schritte:\")\n",
    "for quarter, tasks in phase3_summary['next_steps'].items():\n",
    "    print(f\"  {quarter}: {len(tasks)} Tasks geplant\")\n",
    "print(\"\\nüéØ Das CiviCRM-Automation-System ist nun vollst√§ndig skaliert, intelligent und governance-ready!\")\n",
    "print(\"üí° Alle Komponenten sind live, getestet und bereit f√ºr den produktiven Einsatz.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
